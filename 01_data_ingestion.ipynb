{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d2bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import dask\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import LocalCluster\n",
    "import os\n",
    "from paths import ROOT, YEARS, TASK1_OUT_ROOT, get_files, TAXI_ZONES_SHAPEFILE, ZONES_TO_CENTROIDS_MAPPING_CSV, TASK1_SCHEMA, TASK1_NP_SCHEMA\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# Local cluster\n",
    "# cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit=\"12GB\")\n",
    "# client = cluster.get_client()\n",
    "# print(cluster.dashboard_link)\n",
    "\n",
    "column_consistency_mapping = {\n",
    "    \"End_Lat\": \"dropoff_latitude\",\n",
    "    \"End_Lon\": \"dropoff_longitude\",\n",
    "    \"Start_Lat\": \"pickup_latitude\",\n",
    "    \"Start_Lon\": \"pickup_longitude\",\n",
    "    \"Fare_Amt\": \"fare_amount\",\n",
    "    \"Tip_Amt\": \"tip_amount\",\n",
    "    \"Tolls_Amt\": \"tolls_amount\",\n",
    "    \"Total_Amt\": \"total_amount\",\n",
    "    \"Passenger_Count\": \"passenger_count\",\n",
    "    \"Payment_Type\": \"payment_type\",\n",
    "    \"Rate_Code\": \"rate_code_id\",\n",
    "    \"rate_code\": \"rate_code_id\",\n",
    "    \"RatecodeID\": \"rate_code_id\",\n",
    "    \"Trip_Distance\": \"trip_distance\",\n",
    "    \"Trip_Dropoff_DateTime\": \"tpep_dropoff_datetime\",\n",
    "    \"Trip_Pickup_DateTime\": \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "    \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "    \"Airport_fee\": \"airport_fee\",\n",
    "    \"VendorID\": \"vendor_id\",\n",
    "    \"vendor_name\": \"vendor_id\",\n",
    "    \"surcharge\": \"extra\",\n",
    "    \"store_and_forward\": \"store_and_fwd_flag\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca09e8",
   "metadata": {},
   "source": [
    "### Renaming columns for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c5a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sum((get_files(ROOT, year) for year in YEARS), [])\n",
    "dfs = [dd.read_parquet(f) for f in files]\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    # Rename columns\n",
    "    for old_col, new_col in column_consistency_mapping.items():\n",
    "        if old_col in dfs[i].columns.tolist():\n",
    "            dfs[i] = dfs[i].rename(columns={old_col: new_col})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1502b",
   "metadata": {},
   "source": [
    "### Fix column values (mapping, dtypes...) to be able to concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057617d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_vendor(value):\n",
    "    # vendor_id (previously vendor_name but stands for the same thing)\n",
    "    _old_vendorname2id = {\n",
    "        \"CMT\": 1,\n",
    "        \"DDS\": 2, # DDS and VTS were merged (Verifone) and later also merged with Curb Mobility\n",
    "        \"VTS\": 2,\n",
    "    }\n",
    "    valid_vendor_ids= [1, 2, 6, 7]\n",
    "    if str(value) in _old_vendorname2id:\n",
    "        return _old_vendorname2id[value]\n",
    "    if float(value) in valid_vendor_ids:\n",
    "        return int(value)\n",
    "    return -1 # Invalid vendor id\n",
    "    \n",
    "def map_rate_code(value):\n",
    "    # rate_code_id\n",
    "    # 1-6 and 99 (missing or unknown). All other values are set to 99\n",
    "    valid_rate_code_ids = [1, 2, 3, 4, 5, 6, 99]\n",
    "    if float(value) in valid_rate_code_ids:\n",
    "        return int(value)\n",
    "    return 99 # Invalid rate code id\n",
    "    \n",
    "def map_store_and_fwd_flag(value):\n",
    "    # store_and_fwd_flag\n",
    "    # npnan to \"NA\"\n",
    "    letter_mapping = {\n",
    "        \"Y\": 1,\n",
    "        \"N\": 0,\n",
    "    }\n",
    "    if str(value).strip() in letter_mapping:\n",
    "        return letter_mapping[str(value)]\n",
    "    try:\n",
    "        v = int(float(value))\n",
    "        if v in [0, 1]:\n",
    "            return v\n",
    "    except:\n",
    "        pass\n",
    "    return -1\n",
    "\n",
    "def map_payment_type(value):\n",
    "    # ignore case and strip\n",
    "    payment_type_mapping = {\n",
    "        \"credit\": 1,\n",
    "        \"crd\": 1,\n",
    "        \"cre\":1,\n",
    "        \"cash\": 2,\n",
    "        \"csh\": 2,\n",
    "        \"cas\": 2,\n",
    "        \"noc\": 3,\n",
    "        \"no charge\": 3,\n",
    "        \"no\": 3,\n",
    "        \"dis\": 4,\n",
    "        \"dispute\": 4,\n",
    "        \"na\": 5,\n",
    "    }\n",
    "    valid_vals = [0, 1, 2, 3, 4, 5, 6]\n",
    "    if str(value).strip().lower() in payment_type_mapping:\n",
    "        return payment_type_mapping[str(value).strip().lower()]\n",
    "    if float(value) in valid_vals:\n",
    "        return int(float(value))\n",
    "    return 5 # unknown\n",
    "\n",
    "def get_locationid_to_centroid(shapefile):\n",
    "    # Load zones as GeoDataFrame\n",
    "    zones = gpd.read_file(shapefile)\n",
    "\n",
    "    # Reproject to NYC's local projected CRS (US feet) for correct geometry math\n",
    "    zones_projected = zones.to_crs(\"EPSG:2263\")\n",
    "\n",
    "    # Calculate centroid in projected space\n",
    "    zones_projected['centroid'] = zones_projected.geometry.centroid\n",
    "\n",
    "    # Convert centroid back to WGS84 (lat/lon)\n",
    "    centroids_wgs84 = zones_projected.set_geometry('centroid').to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Extract lat/lng from centroid geometry\n",
    "    zones['centroid_lat'] = centroids_wgs84.geometry.y\n",
    "    zones['centroid_lng'] = centroids_wgs84.geometry.x\n",
    "\n",
    "    zones = zones[['LocationID', 'centroid_lat', 'centroid_lng']]\n",
    "    zones['centroid_lat'] = zones['centroid_lat'].astype(float)\n",
    "    zones['centroid_lng'] = zones['centroid_lng'].astype(float)\n",
    "    zones['LocationID'] = zones['LocationID'].astype(int)\n",
    "\n",
    "    return zones\n",
    "\n",
    "\n",
    "locationid_to_centers_df = get_locationid_to_centroid(TAXI_ZONES_SHAPEFILE).sort_values(by='LocationID', ascending=True, ignore_index=True)\n",
    "# save to base dir\n",
    "locationid_to_centers_df.to_csv(ZONES_TO_CENTROIDS_MAPPING_CSV, index=False)\n",
    "\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    # map vendor_id column but with dask so i can compute later\n",
    "    dfs[i]['vendor_id'] = dfs[i]['vendor_id'].map(map_vendor, meta=('vendor_id', 'int8'))\n",
    "    dfs[i]['rate_code_id'] = dfs[i]['rate_code_id'].map(map_rate_code, meta=('rate_code_id', 'int8'))\n",
    "    dfs[i]['store_and_fwd_flag'] = dfs[i]['store_and_fwd_flag'].map(map_store_and_fwd_flag, meta=('store_and_fwd_flag', 'int8'))\n",
    "    dfs[i]['payment_type'] = dfs[i]['payment_type'].map(map_payment_type, meta=('payment_type', 'int8'))\n",
    "    # pickup\n",
    "    if \"PULocationID\" in dfs[i].columns:\n",
    "        # set dtype to int\n",
    "        dfs[i]['PULocationID'] = dfs[i]['PULocationID'].astype(int)\n",
    "        dfs[i] = dfs[i].merge(locationid_to_centers_df, how='left', left_on='PULocationID', right_on='LocationID')\n",
    "        # rename to pickup lattitude and longitude\n",
    "        dfs[i] = dfs[i].rename(columns={'centroid_lat': 'pickup_latitude', 'centroid_lng': 'pickup_longitude'})\n",
    "\n",
    "    # dropoff\n",
    "    if \"DOLocationID\" in dfs[i].columns:\n",
    "        # set dtype to int\n",
    "        dfs[i]['DOLocationID'] = dfs[i]['DOLocationID'].astype(int)\n",
    "        dfs[i] = dfs[i].merge(locationid_to_centers_df, how='left', left_on='DOLocationID', right_on='LocationID')\n",
    "        # rename to dropoff lattitude and longitude\n",
    "        dfs[i] = dfs[i].rename(columns={'centroid_lat': 'dropoff_latitude', 'centroid_lng': 'dropoff_longitude'})\n",
    "    # drop LocationID columns\n",
    "    dfs[i] = dfs[i].drop(columns=['DOLocationID', 'PULocationID', 'LocationID_x', 'LocationID_y'], errors='ignore')\n",
    "\n",
    "    if 'airport_fee' not in dfs[i].columns:\n",
    "        # set airport_fee to 0\n",
    "        dfs[i]['airport_fee'] = 0.0\n",
    "    if 'congestion_surcharge' not in dfs[i].columns:\n",
    "        dfs[i]['congestion_surcharge'] = 0.0\n",
    "    if 'improvement_surcharge' not in dfs[i].columns:\n",
    "        dfs[i]['improvement_surcharge'] = 0.0\n",
    "    year = int(os.path.basename(f).split('_')[2][:4])\n",
    "    dfs[i]['year'] = year\n",
    "    dfs[i]['year'] = dfs[i]['year'].astype(np.int16)\n",
    "    dfs[i]['mta_tax'] = dfs[i]['mta_tax'].fillna(0.0)\n",
    "    # parse datetimes\n",
    "    dfs[i]['pickup_datetime'] = dd.to_datetime(dfs[i]['pickup_datetime'], errors='raise')\n",
    "    dfs[i]['dropoff_datetime'] = dd.to_datetime(dfs[i]['dropoff_datetime'], errors='raise')\n",
    "    dfs[i]['airport_fee'] = dfs[i]['airport_fee'].fillna(0.0).astype(float)\n",
    "    dfs[i]['congestion_surcharge'] = dfs[i]['congestion_surcharge'].fillna(0.0).astype(float)\n",
    "    dfs[i]['improvement_surcharge'] = dfs[i]['improvement_surcharge'].fillna(0.0).astype(float)\n",
    "    dfs[i]['extra'] = dfs[i]['extra'].fillna(0.0).astype(float)\n",
    "    dfs[i]['passenger_count'] = dfs[i]['passenger_count'].fillna(0).astype(int)\n",
    "    # Fix dtypes\n",
    "    dfs[i] = dfs[i].astype(TASK1_NP_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15b43e4",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47adabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(TASK1_OUT_ROOT, \"one_year\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TASK1_OUT_ROOT, \"five_years\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TASK1_OUT_ROOT, \"all\"), exist_ok=True)\n",
    "jobs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872289cb",
   "metadata": {},
   "source": [
    "#### Parquet (all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc68fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[-1].to_parquet( # parquet can append only if a dataset already exists.\n",
    "    path=os.path.join(TASK1_OUT_ROOT, \"all\"),\n",
    "    partition_on=['year'],\n",
    "    engine='pyarrow',\n",
    "    schema=TASK1_SCHEMA,\n",
    "    write_index=False,\n",
    "    compute=True,\n",
    ")\n",
    "for i in list(range(len(dfs) - 1))[:LIMIT]:\n",
    "    jobs.append(dfs[i].to_parquet(\n",
    "        path=os.path.join(TASK1_OUT_ROOT, \"all\"),\n",
    "        partition_on=['year'],\n",
    "        engine='pyarrow',\n",
    "        schema=TASK1_SCHEMA,\n",
    "        write_index=False,\n",
    "        append=True,\n",
    "        compute=False\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b50bc",
   "metadata": {},
   "source": [
    "#### CSV (5 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae58287",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(-61, -1, 1)): # the last df is Jan 2025..., select last 5 years before 2025 (60 months)\n",
    "    jobs.append(dfs[i].to_csv(\n",
    "        filename=os.path.join(TASK1_OUT_ROOT, \"five_years\", \"2020_2024.csv\"),\n",
    "        single_file=True,\n",
    "        index=False,\n",
    "        mode=\"at\", # append/create if not exists\n",
    "        compute=False\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c27204",
   "metadata": {},
   "source": [
    "#### CSV (1 year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5951efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(-13, -1, 1))[:LIMIT]: # last year before 2025\n",
    "    jobs.append(dfs[i].to_csv(\n",
    "        filename=os.path.join(TASK1_OUT_ROOT, \"one_year\", \"2024.csv\"),\n",
    "        single_file=True,\n",
    "        index=False,\n",
    "        mode=\"at\", # append/create if not exists\n",
    "        compute=False\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11395106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, ['/var/home/kvovk/work/bd-project-25/data/task1/one_year/2024.csv'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.compute(*jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba5201",
   "metadata": {},
   "source": [
    "#### HDF (1 year)\n",
    "- using h5py as dask's implementation has problems (it can only save in the \"tables\" format resulting in a larger file than the equivalent CSV...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b7b1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.concat(dfs[-13:-1], axis=0)\n",
    "# convert datetimes to int64 (h5py can't handle datetimes directly)\n",
    "for col in df.select_dtypes(include=['datetime64[ns]']):\n",
    "    df[col] = df[col].astype('int64')\n",
    "\n",
    "df = df.compute()\n",
    "# to numpy (structured array) as h5py can't handle dataframes directly\n",
    "dtype = np.dtype([(col, df[col].dtype) for col in df.columns])\n",
    "structured_array = np.empty(len(df), dtype=dtype)\n",
    "for col in df.columns:\n",
    "    structured_array[col] = df[col].to_numpy()\n",
    "\n",
    "# write as single dataset\n",
    "with h5py.File(os.path.join(TASK1_OUT_ROOT, \"one_year\", \"2024.h5\"), 'w') as h5f:\n",
    "    h5f.create_dataset('taxidata', data=structured_array, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
