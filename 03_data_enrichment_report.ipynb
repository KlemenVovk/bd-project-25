{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import dask.dataframe as dd\n",
    "from shapely.geometry import Point, shape\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET ENRICHMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part of the task was to enrich the dataset with additional information about:\n",
    "- weather (based on the time and location)\n",
    "- Schools in vicinity (based on location)\n",
    "- Events (based on time and location)\n",
    "- Businesses in vicinity (based on location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSINESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we collect the business data from the followin source: 'https://data.cityofnewyork.us/Business/Issued-Licenses/w7w3-xahh/about_data'. The source contains various information about businesses in NYC, however, the data that we're interested in is mainly the name of the business and the location. Ideally, we would like to have the location information in the form of coordinates, however, a lot of entries in the dataset are missing that information. In order to get around that problem, we use the ZIP Code information to infer approximate location. We use a dataset obtained from 'https://download.geonames.org/export/zip/' to map various US ZIP codes to shape files, from which we can then calculate center points. The two datasets are then joined on the zipcodes and missing coordinates are filled with calculated approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZIP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zip_data(PATH):\n",
    "    dtype = {\n",
    "        1: 'int64',   # ZIP Code\n",
    "        4: 'object',  # State\n",
    "        9: 'float64', # lat\n",
    "        10: 'float64' # lng\n",
    "    }\n",
    "    zip = dd.read_csv(PATH, sep='\\t', header=None, dtype=dtype, usecols=[1, 4, 9, 10])\n",
    "    zip.columns = ['ZIP Code', 'State', 'lat', 'lng']\n",
    "    return zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_PATH = 'data/US.txt'\n",
    "zip_df = load_zip_data(ZIP_PATH)\n",
    "# zip_df = zip_df.compute()\n",
    "# print(zip_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_business_data(PATH):\n",
    "    bsns = dd.read_csv(\n",
    "    PATH,\n",
    "    usecols=['Business Name', 'ZIP Code', 'Latitude', 'Longitude'],\n",
    "    dtype={'ZIP Code': 'str', # Some of the entries are not numeric, manual conversion needed\n",
    "           'Business Name': 'object',\n",
    "           'Longitude': 'float64',\n",
    "           'Latitude': 'float64'}  # still useful to enforce type\n",
    ")\n",
    "\n",
    "    bsns['ZIP Code'] = dd.to_numeric(bsns['ZIP Code'], errors='coerce')\n",
    "    bsns = bsns.dropna(subset=['ZIP Code'])\n",
    "    bsns['ZIP Code'] = bsns['ZIP Code'].astype('int64') # This has to be enforced to ensure merge works\n",
    "    return bsns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUSINESS_PATH = 'data/businesses.csv'\n",
    "bsns_gdf = load_business_data(BUSINESS_PATH)\n",
    "# bsns_gdf = bsns_dgf.compute()\n",
    "# print(bsns_gdf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge business and zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_business_zip(bsns, zip_ddf):\n",
    "    bsns = bsns.merge(zip_ddf, how='left', on='ZIP Code')\n",
    "    bsns['Latitude'] = bsns['Latitude'].fillna(bsns['lat'])\n",
    "    bsns['Longitude'] = bsns['Longitude'].fillna(bsns['lng'])\n",
    "    bsns = bsns.drop(columns=['lat', 'lng', 'State', 'ZIP Code'])\n",
    "    bsns = bsns.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "    def make_point(lon, lat):\n",
    "        return Point(lon, lat)\n",
    "\n",
    "\n",
    "    # Apply the make_point function to create a 'geometry' column\n",
    "\n",
    "    bsns = bsns.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            geometry=df.apply(lambda x: make_point(x['Longitude'], x['Latitude']), axis=1)\n",
    "        ), meta={'Business Name': 'str', 'Latitude': 'float64', 'Longitude': 'float64', 'geometry': 'geometry'} # Dask needs to know the type of the new column\n",
    "    )\n",
    "\n",
    "    bsns = dgpd.from_dask_dataframe(bsns, geometry='geometry')\n",
    "    bsns = bsns.set_crs(\"EPSG:4326\").to_crs(3857)\n",
    "    bsns = bsns.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "    return bsns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Business Name                          geometry\n",
      "0             Denis Spedalieri  POINT (-8235727.623 4963332.949)\n",
      "1  SANJAY'S VARIETY STORE INC.  POINT (-8211964.586 4969967.308)\n",
      "2                 Gayla Hibner    POINT (-8237453.076 4971936.9)\n",
      "3    FAMILY CARE REFERRAL, LLC   POINT (-8235658.16 4976635.563)\n",
      "4                   Donna Hill  POINT (-8228124.502 4962687.224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sancho/miniconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/dask_expr/_expr.py:1543: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return get_meta_library(args[0]).to_datetime(*args, **kwargs)\n",
      "/home/sancho/miniconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/dask_expr/_expr.py:1543: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return get_meta_library(args[0]).to_datetime(*args, **kwargs)\n",
      "/home/sancho/miniconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/dask_expr/_expr.py:1543: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return get_meta_library(args[0]).to_datetime(*args, **kwargs)\n",
      "/home/sancho/miniconda3/envs/bigData/lib/python3.11/site-packages/dask/dataframe/dask_expr/_expr.py:1543: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return get_meta_library(args[0]).to_datetime(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "bsns = merge_business_zip(bsns_gdf, zip_df)\n",
    "bsns = bsns.compute()\n",
    "print(bsns.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEATHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we weren't able to find a data source that would cover the entire timespan and would at the same time have high spatial granularity. In the end, we opted for 'https://open-meteo.com/en/docs/historical-weather-api', which provides hourly weather data for the entire New York. From the available information we took the temperature and the weather code, and downloaded data for the entire required time period with one API call. We made sure to adjust for the timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weather_data(PATH):\n",
    "    weather = dd.read_csv(PATH)\n",
    "    \n",
    "    # Handle datetime operations\n",
    "    weather['time'] = dd.to_datetime(weather['time'])\n",
    "    weather['time'] = weather['time'].dt.tz_localize('GMT').dt.tz_convert('America/New_York')\n",
    "    weather['time'] = weather['time'].dt.tz_localize(None)\n",
    "    \n",
    "    # Convert columns to appropriate types\n",
    "    weather['Temperature'] = weather['Temperature'].astype(float)\n",
    "    weather['Weather Code'] = weather['Weather Code'].astype(int)\n",
    "    \n",
    "    return weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  Temperature  Weather Code\n",
      "0 2007-12-31 19:00:00          1.6             0\n",
      "1 2007-12-31 20:00:00          0.3             0\n",
      "2 2007-12-31 21:00:00          0.3             0\n",
      "3 2007-12-31 22:00:00         -0.3             0\n",
      "4 2007-12-31 23:00:00         -0.6             0\n"
     ]
    }
   ],
   "source": [
    "WEATHER_PATH = 'data/weather_data.csv'\n",
    "weather_ddf = load_weather_data(WEATHER_PATH)\n",
    "weather_gdf = weather_ddf.compute()\n",
    "print(weather_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCHOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the school data from the following source: 'https://data.cityofnewyork.us/Education/2019-2020-School-Locations/wg9x-4ke6/data_preview'. Since vast majority of entries came equipped with exact coordinates, no much preprocessing was required. We kept the location and the name information and discarded the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_schools_data(PATH):\n",
    "    dtypes = {\n",
    "        'nta_name': 'str',\n",
    "        'location_category_description': 'str',\n",
    "        'latitude': 'float64',\n",
    "        'longitude': 'float64'\n",
    "    }\n",
    "    schools = dd.read_csv(PATH, usecols=['nta_name', 'location_category_description', 'latitude', 'longitude'], dtype=dtypes)\n",
    "    \n",
    "    # Combine name columns\n",
    "    schools['Name'] = schools['nta_name'].astype(str) + ' ' + schools['location_category_description'].astype(str)\n",
    "    \n",
    "    # Only keep relevant columns\n",
    "    schools = schools[['Name', 'latitude', 'longitude']]\n",
    "    \n",
    "    # Create geometry column in Dask\n",
    "    def make_point(row):\n",
    "        return Point(row['longitude'], row['latitude'])\n",
    "\n",
    "    # Use map_partitions for custom geometry creation\n",
    "    schools['Location'] = schools.map_partitions(lambda df: df.apply(make_point, axis=1), meta=('Location', 'geometry'))\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    schools_gdf = dgpd.from_dask_dataframe(schools, geometry='Location')\n",
    "    schools_gdf = schools_gdf.set_crs(\"EPSG:4326\").to_crs(3857)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    schools_gdf = schools_gdf.drop(columns=['longitude', 'latitude'])\n",
    "    \n",
    "    return schools_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Name  \\\n",
      "0                        Sunset Park West Elementary   \n",
      "1  Prospect Lefferts Gardens-Wingate Junior High-...   \n",
      "2                            Clinton Hill Elementary   \n",
      "3                           East New York Elementary   \n",
      "4                      Stuyvesant Heights Elementary   \n",
      "\n",
      "                           Location  \n",
      "0  POINT (-8238913.587 4960700.272)  \n",
      "1   POINT (-8232251.672 4961795.46)  \n",
      "2  POINT (-8232657.321 4965594.938)  \n",
      "3  POINT (-8224203.384 4962100.238)  \n",
      "4  POINT (-8228956.059 4966025.055)  \n"
     ]
    }
   ],
   "source": [
    "SCHOOLS_PATH = 'data/schools.csv'\n",
    "school_ddf = load_schools_data(SCHOOLS_PATH)\n",
    "school_gdf = school_ddf.compute()\n",
    "print(school_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the event information was obtained from the following source: 'https://data.cityofnewyork.us/City-Government/NYC-Permitted-Event-Information-Historical/bkfu-528j/about_data'. Since this time, no information about exact location was included, we had to infer location based on the police precinct, responsible for the event. We found a mapping from police precincts to shape files here: 'https://data.cityofnewyork.us/City-Government/Police-Precincts/y76i-bdw7/about_data'. Once again, we calculated the centroids for each precinct, joined the two dataset on police precincts, and used this information as our best estimation of event location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precincts_data(PATH):\n",
    "    precincts = dd.read_csv(PATH, usecols=['precinct', 'the_geom'], dtype={'precinct': 'int64', 'the_geom': 'object'})\n",
    "    \n",
    "    # Apply ast.literal_eval on the 'the_geom' column\n",
    "    precincts['the_geom'] = precincts['the_geom'].apply(ast.literal_eval, meta=('x', 'object'))\n",
    "    \n",
    "    # Create geometry using map_partitions\n",
    "    precincts['geometry'] = precincts.map_partitions(lambda df: df['the_geom'].apply(shape), meta=('geometry', 'geometry'))\n",
    "    precincts = precincts.drop(columns=['the_geom'])\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf = dgpd.from_dask_dataframe(precincts, geometry='geometry')\n",
    "     \n",
    "    # Calculate centroids\n",
    "    gdf['centroid'] = gdf.geometry.centroid\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    gdf = gdf.drop(columns=['geometry'])\n",
    "    \n",
    "    # Ensure 'precinct' column is of integer type\n",
    "    gdf['precinct'] = gdf['precinct'].astype(int)\n",
    "    \n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precinct                    centroid\n",
      "0         1  POINT (-74.01207 40.70979)\n",
      "1         5  POINT (-73.99715 40.71641)\n",
      "2         6  POINT (-74.00238 40.73367)\n",
      "3         7  POINT (-73.98395 40.71544)\n",
      "4         9  POINT (-73.98339 40.72627)\n"
     ]
    }
   ],
   "source": [
    "PRECINCTS_PATH = 'data/police_precincts.csv'\n",
    "precincts = load_precincts_data(PRECINCTS_PATH)\n",
    "precincts = precincts.compute()\n",
    "print(precincts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preload evets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_events_data(PATH):\n",
    "    events = dd.read_csv(PATH, usecols=['Event Name', 'Start Date/Time', 'End Date/Time', 'Police Precinct'], dtype={'Event Name': 'str', 'Start Date/Time': 'str', 'End Date/Time': 'str', 'Police Precinct': 'str'})\n",
    "    # Columns selection and transformations\n",
    "    events = events[['Event Name', 'Start Date/Time', 'End Date/Time', 'Police Precinct']]\n",
    "    events['Police Precinct'] = events['Police Precinct'].apply(lambda x: str(x).split(',')[0], meta=('x', 'str'))\n",
    "    events['Police Precinct'] = dd.to_numeric(events['Police Precinct'], errors='coerce')\n",
    "    events['Police Precinct'] = events['Police Precinct'].astype('Int64')  # Use Int64 for nullable integers\n",
    "\n",
    "    \n",
    "    # Handle datetime\n",
    "    events['Start Date/Time'] = dd.to_datetime(events['Start Date/Time'])\n",
    "    events['End Date/Time'] = dd.to_datetime(events['End Date/Time'])\n",
    "    \n",
    "    return events\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Event Name     Start Date/Time  \\\n",
      "0                                  Big Apple Circus 2017-11-18 19:00:00   \n",
      "1                          Mt. Eden Farmer's Market 2017-11-16 08:00:00   \n",
      "2                    Columbia  Greenmarket Thursday 2017-11-21 08:00:00   \n",
      "3                                  Lawn Maintenance 2017-11-23 00:00:00   \n",
      "4  October, November December model aircraft flying 2017-11-22 09:00:00   \n",
      "\n",
      "        End Date/Time  Police Precinct  \n",
      "0 2017-11-18 20:00:00               20  \n",
      "1 2017-11-16 16:00:00               44  \n",
      "2 2017-11-21 17:00:00               26  \n",
      "3 2017-11-23 23:58:00               13  \n",
      "4 2017-11-22 20:00:00              122  \n"
     ]
    }
   ],
   "source": [
    "EVENTS_PATH = 'data/events_sample.csv'\n",
    "events = load_events_data(EVENTS_PATH)\n",
    "events = events.compute()\n",
    "print(events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Precincts and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_events_precincts(events, precincts):\n",
    "    events['Police Precinct'] = events['Police Precinct'].astype('Int64')  # Ensure the type matches\n",
    "    precincts['precinct'] = precincts['precinct'].astype('Int64')  # Ensure the type matches\n",
    "    precincts = precincts.set_geometry('centroid')\n",
    "    precincts = precincts.set_crs(\"EPSG:4326\")\n",
    "    merged = dd.merge(events, precincts, how='left', left_on='Police Precinct', right_on='precinct')\n",
    "    merged = merged.drop(columns=['precinct', 'Police Precinct'])\n",
    "    merged = merged.rename(columns={'centroid': 'Location'})\n",
    "    merged = dgpd.from_dask_dataframe(merged, geometry='Location')\n",
    "    merged = merged.set_crs(\"EPSG:4326\").to_crs(3857)\n",
    "\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Event Name     Start Date/Time  \\\n",
      "0                                  Big Apple Circus 2017-11-18 19:00:00   \n",
      "1                          Mt. Eden Farmer's Market 2017-11-16 08:00:00   \n",
      "2                    Columbia  Greenmarket Thursday 2017-11-21 08:00:00   \n",
      "3                                  Lawn Maintenance 2017-11-23 00:00:00   \n",
      "4  October, November December model aircraft flying 2017-11-22 09:00:00   \n",
      "\n",
      "        End Date/Time                          Location  \n",
      "0 2017-11-18 20:00:00  POINT (-8235604.809 4979855.952)  \n",
      "1 2017-11-16 16:00:00   POINT (-8228825.29 4987858.923)  \n",
      "2 2017-11-21 17:00:00  POINT (-8233067.187 4984649.037)  \n",
      "3 2017-11-23 23:58:00  POINT (-8235910.115 4973779.129)  \n",
      "4 2017-11-22 20:00:00    POINT (-8250545.9 4949933.985)  \n"
     ]
    }
   ],
   "source": [
    "precincts = load_precincts_data(PRECINCTS_PATH)\n",
    "events = load_events_data(EVENTS_PATH)\n",
    "merged = merge_events_precincts(events, precincts)\n",
    "merged = merged.compute()\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN WITH ORIGINAL DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest part of data enrichment task came in the form of joining. \n",
    "\n",
    "Firstly, in order to do any kind of spatial calculations, a regular dataframe had to be converted into a geo-dataframe with a designated geometry column. However, since each geo-dataframe can only have one single geometry column (our dataset would require 2 - one for pickup and one for dropoff location), we had to create two separate geodataframes. For the purposes of proving the concept, the rest of the calculations were only made on the dropoff location based geodataframe.\n",
    "\n",
    "Join with the weather data was relatively straight forward. Timestamps from the original dataset were rounded to hours since that was the granularity of our weather information. After that, a simple join was performed.\n",
    "\n",
    "Schools and businesses were a bit trickier. While in vanilla pandas, a simple *sjoin_nearest* could be performed, dask version of geopandas doesn't support that operation, since by default it can't be parallelized. In order to find the closest school/business, the entire dataset has to be loaded into memory. The problem was solved using the cKDTree algorithm, which uses a binary tree-like algorith, to find the closest neighbor effitiently. And while the business/school data had to be loaded into memory, the original dataset can be processed in batches.\n",
    "\n",
    "Lastly, we had to join the events data. This task proved to be the most tedeous, since the join had to take into account both spatial and temporal conditions. We tackled the problem by first dealing with the spatial part. We first decided on an arbitrary vicinity condition (1km for example). We created a buffer around each taxi PU/DO location of that size and then performed a spatial join with the business dataset. However, since dask implementation currently only supports inner joins, another join with the original dataset was required in order to retain information about taxi trips, that did not end in a vicinity of an event. With that, we got a dataset containing one row for each trip-event combination, that were within the set distance, and one row for each trip that didn't end in a vicinity of an event. To enforce the temporal condition, we calculated a new time difference column, which recorder the time difference between event  and taxi trip. We then filtered the entire dataset based on another arbitrary temporal condition (1h for example). This was, we ended up with a dataset, the contained one row for each taxi trip-event combination, where the event started within 1km and 2hours of the taxi trip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>rate_code_id</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>...</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Weather Code</th>\n",
       "      <th>dist_from_school</th>\n",
       "      <th>nearest_school_name</th>\n",
       "      <th>dist_from_business</th>\n",
       "      <th>nearest_business_name</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>DO Location</th>\n",
       "      <th>time_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-12-01 00:12:27</td>\n",
       "      <td>2024-12-01 00:31:12</td>\n",
       "      <td>1</td>\n",
       "      <td>9.76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.695797</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>769.353141</td>\n",
       "      <td>Brooklyn Heights-Cobble Hill K-8</td>\n",
       "      <td>70.814723</td>\n",
       "      <td>PDH DESIGN AND CONSTRUCTION LLC</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POINT (-8237113.205 4967574.804)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-11-30 23:56:04</td>\n",
       "      <td>2024-12-01 00:28:15</td>\n",
       "      <td>1</td>\n",
       "      <td>7.62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.299999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.818256</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>435.805406</td>\n",
       "      <td>Central Harlem North-Polo Grounds Elementary</td>\n",
       "      <td>77.150995</td>\n",
       "      <td>2400 DELI &amp; GROCERY CORPORATION</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POINT (-8231049.2 4985571.368)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-12-01 00:50:35</td>\n",
       "      <td>2024-12-01 01:24:46</td>\n",
       "      <td>4</td>\n",
       "      <td>20.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.780437</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>166.127908</td>\n",
       "      <td>Upper East Side-Carnegie Hill K-12 all grades</td>\n",
       "      <td>101.501429</td>\n",
       "      <td>1065 PARK AVENUE GARAGE LLC</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POINT (-8232856.511 4980009.983)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-12-01 00:18:16</td>\n",
       "      <td>2024-12-01 00:33:16</td>\n",
       "      <td>3</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>261.694336</td>\n",
       "      <td>Midtown-Midtown South High school</td>\n",
       "      <td>19.407107</td>\n",
       "      <td>ADS CONSTRUCTION CORP.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POINT (-8236800.662 4975315.508)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-12-01 00:56:13</td>\n",
       "      <td>2024-12-01 01:18:25</td>\n",
       "      <td>1</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.799999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.715370</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>665.810341</td>\n",
       "      <td>Bushwick South High school</td>\n",
       "      <td>273.248151</td>\n",
       "      <td>T.B.I. LLC</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POINT (-8230605.866 4970449.047)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor_id     pickup_datetime    dropoff_datetime  passenger_count  \\\n",
       "0          2 2024-12-01 00:12:27 2024-12-01 00:31:12                1   \n",
       "1          2 2024-11-30 23:56:04 2024-12-01 00:28:15                1   \n",
       "2          2 2024-12-01 00:50:35 2024-12-01 01:24:46                4   \n",
       "3          2 2024-12-01 00:18:16 2024-12-01 00:33:16                3   \n",
       "4          2 2024-12-01 00:56:13 2024-12-01 01:18:25                1   \n",
       "\n",
       "   trip_distance  rate_code_id  store_and_fwd_flag  payment_type  fare_amount  \\\n",
       "0           9.76             1                   0             1    38.000000   \n",
       "1           7.62             1                   0             1    37.299999   \n",
       "2          20.07             2                   0             2    70.000000   \n",
       "3           2.34             1                   0             1    15.600000   \n",
       "4           5.05             1                   0             1    26.799999   \n",
       "\n",
       "   extra  ...  dropoff_latitude  Temperature  Weather Code  dist_from_school  \\\n",
       "0    6.0  ...         40.695797         -2.8             0        769.353141   \n",
       "1    1.0  ...         40.818256         -2.5             0        435.805406   \n",
       "2    0.0  ...         40.780437         -2.8             0        166.127908   \n",
       "3    1.0  ...         40.748497         -2.8             0        261.694336   \n",
       "4    1.0  ...         40.715370         -2.8             0        665.810341   \n",
       "\n",
       "                             nearest_school_name  dist_from_business  \\\n",
       "0               Brooklyn Heights-Cobble Hill K-8           70.814723   \n",
       "1   Central Harlem North-Polo Grounds Elementary           77.150995   \n",
       "2  Upper East Side-Carnegie Hill K-12 all grades          101.501429   \n",
       "3              Midtown-Midtown South High school           19.407107   \n",
       "4                     Bushwick South High school          273.248151   \n",
       "\n",
       "             nearest_business_name  Event Name  \\\n",
       "0  PDH DESIGN AND CONSTRUCTION LLC        <NA>   \n",
       "1  2400 DELI & GROCERY CORPORATION        <NA>   \n",
       "2      1065 PARK AVENUE GARAGE LLC        <NA>   \n",
       "3           ADS CONSTRUCTION CORP.        <NA>   \n",
       "4                       T.B.I. LLC        <NA>   \n",
       "\n",
       "                        DO Location  time_diff  \n",
       "0  POINT (-8237113.205 4967574.804)        NaN  \n",
       "1    POINT (-8231049.2 4985571.368)        NaN  \n",
       "2  POINT (-8232856.511 4980009.983)        NaN  \n",
       "3  POINT (-8236800.662 4975315.508)        NaN  \n",
       "4  POINT (-8230605.866 4970449.047)        NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"  Read and prepare the data \"\"\"\n",
    "\n",
    "DATA_PATH = 'data/example.parquet'\n",
    "df = dd.read_parquet(DATA_PATH, engine='pyarrow')\n",
    "\n",
    "df = df.dropna(subset=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'])\n",
    "df['PU Location'] = df.apply(lambda r: Point(r['pickup_longitude'], r['pickup_latitude']), axis=1, meta=('PU Lucation', 'geometry'))\n",
    "df['DO Location'] = df.apply(lambda r: Point(r['dropoff_longitude'], r['dropoff_latitude']), axis=1, meta=('DO Lucation', 'geometry'))\n",
    "\n",
    "# pickup_gdf = dgpd.from_dask_dataframe(df, geometry='PU Location')\n",
    "# pickup_gdf = pickup_gdf.drop(columns=['DO Location'])\n",
    "# pickup_gdf = pickup_gdf.set_crs(\"EPSG:4326\").to_crs(3857)\n",
    "\n",
    "dataset = dgpd.from_dask_dataframe(df, geometry='DO Location')\n",
    "dataset = dataset.drop(columns=['PU Location'])\n",
    "dataset = dataset.set_crs(\"EPSG:4326\").to_crs(3857)\n",
    "\n",
    "\n",
    "\"\"\" Join the data with the weather data \"\"\"\n",
    "\n",
    "WEATHER_PATH = 'data/weather_data.csv'\n",
    "weather_ddf = load_weather_data(WEATHER_PATH)\n",
    "\n",
    "dataset['dropoff_datetime'] = dd.to_datetime(dataset['dropoff_datetime'])\n",
    "dataset['dropoff_time_rounded'] = dataset['dropoff_datetime'].dt.round('h')\n",
    "\n",
    "dataset = dataset.merge(weather_ddf, how='left', left_on='dropoff_time_rounded', right_on='time')\n",
    "dataset = dataset.drop(columns=['dropoff_time_rounded', 'time'])\n",
    "\n",
    "\"\"\" Join with the schools data - dask geo doesn't have sjoin-nearest (has to be computed in memory) \"\"\"\n",
    "\n",
    "SCHOOLS_PATH = 'data/schools.csv'\n",
    "school_ddf = load_schools_data(SCHOOLS_PATH)\n",
    "\n",
    "dropoff_gdf = dataset.compute()\n",
    "schools_gdf = school_ddf.compute()\n",
    "\n",
    "# Ensure CRS match\n",
    "# dropoff_gdf = dropoff_gdf.to_crs(3857)\n",
    "# schools_gdf = schools_gdf.to_crs(3857)\n",
    "\n",
    "# Extract coordinate arrays\n",
    "dropoff_coords = np.array(list(zip(dropoff_gdf.geometry.x, dropoff_gdf.geometry.y)))\n",
    "school_coords = np.array(list(zip(schools_gdf.geometry.x, schools_gdf.geometry.y)))\n",
    "\n",
    "dropoff_coords = dropoff_coords[~np.isnan(dropoff_coords).any(axis=1)]\n",
    "school_coords = school_coords[~np.isnan(school_coords).any(axis=1)]\n",
    "\n",
    "# Build KDTree and query nearest neighbors\n",
    "tree = cKDTree(school_coords)\n",
    "distances, indices = tree.query(dropoff_coords, k=1)\n",
    "\n",
    "# Get nearest school info\n",
    "nearest_schools = schools_gdf.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "# Add nearest info back to dropoff_gdf\n",
    "dropoff_gdf['dist_from_school'] = distances\n",
    "dropoff_gdf['nearest_school_name'] = nearest_schools['Name'].values\n",
    "\n",
    "\n",
    "# dropoff_gdf = dd.from_pandas(dropoff_gdf)\n",
    "\n",
    "\"\"\" Join with the business data - dask geo doesn't have sjoin-nearest (has to be computed in memory) \"\"\"\n",
    "\n",
    "BUSINESS_PATH = 'data/businesses.csv'\n",
    "bsns = load_business_data(BUSINESS_PATH)\n",
    "zip_df = load_zip_data(ZIP_PATH)\n",
    "bsns_gdf = merge_business_zip(bsns, zip_df)\n",
    "bsns_gdf = bsns_gdf.compute()\n",
    "\n",
    "bsns_coords = np.array(list(zip(bsns_gdf.geometry.x, bsns_gdf.geometry.y)))\n",
    "tree = cKDTree(bsns_coords)\n",
    "distances, indices = tree.query(dropoff_coords, k=1)\n",
    "nearest_bsns = bsns_gdf.iloc[indices].reset_index(drop=True)\n",
    "dropoff_gdf['dist_from_business'] = distances\n",
    "dropoff_gdf['nearest_business_name'] = nearest_bsns['Business Name'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Join with the events data  \"\"\"\n",
    "\n",
    "PRECINCTS_PATH = 'data/police_precincts.csv'\n",
    "EVENTS_PATH = 'data/events_sample.csv'\n",
    "precincts = load_precincts_data(PRECINCTS_PATH)\n",
    "events_pre = load_events_data(EVENTS_PATH)\n",
    "events = merge_events_precincts(events_pre, precincts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start with your GeoDataFrames\n",
    "dropoff_gdf = dropoff_gdf.reset_index(drop=True)\n",
    "dropoff_gdf['row_id'] = dropoff_gdf.index\n",
    "\n",
    "\n",
    "dropoff_buffered = dropoff_gdf.copy()\n",
    "dropoff_buffered['DO Location'] = dropoff_buffered.buffer(100)\n",
    "matches = dgpd.sjoin(dropoff_buffered, merged, predicate='intersects')\n",
    "\n",
    "\n",
    "# Keep a copy of the geometry column\n",
    "dropoff_geometry = dropoff_gdf.geometry\n",
    "matches_geometry = matches.geometry\n",
    "\n",
    "# Convert to Dask DataFrames (dropping geometry temporarily)\n",
    "dropoff_df = dropoff_gdf.drop(columns='DO Location')\n",
    "matches_df = matches.drop(columns='DO Location')\n",
    "\n",
    "\n",
    "event_cols = [col for col in matches_df.columns \n",
    "              if col not in dropoff_df.columns or col == \"row_id\"]\n",
    "matches_df = matches_df[event_cols]\n",
    "\n",
    "# Merge as plain Dask DataFrames\n",
    "joined = dd.merge(dropoff_df, matches_df, how='left', on='row_id')\n",
    "\n",
    "# Reattach the original geometry (optional: you could pick one of them)\n",
    "joined['DO Location'] = dropoff_geometry\n",
    "\n",
    "# Convert back to Dask GeoDataFrame\n",
    "joined_gdf = dgpd.from_dask_dataframe(joined, geometry='DO Location')\n",
    "\n",
    "\n",
    "joined_gdf['time_diff'] = np.abs((joined_gdf['Start Date/Time'] - joined_gdf['dropoff_datetime']).dt.total_seconds() / 3600)\n",
    "joined_gdf = joined_gdf[(joined_gdf['time_diff'] <= 1) | (joined_gdf['time_diff'].isna())]\n",
    "\n",
    "\n",
    "joined_gdf = joined_gdf.drop(columns=['row_id', 'index_right', 'Start Date/Time', 'End Date/Time'])\n",
    "display(joined_gdf.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
