{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fa3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from constants import TASK1_OUT_ROOT, RAW_DATA_ROOT, TASK1_NP_SCHEMA, RESULTS_ROOT\n",
    "import os\n",
    "from glob import glob\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_raw_files(root_path, year):\n",
    "    return sorted(list(glob(f\"{root_path}/yellow_tripdata_{year}*.parquet\")))\n",
    "\n",
    "def get_total_size_GB(paths):\n",
    "    return round(sum(os.path.getsize(p) for p in paths) / (1024 * 1024 * 1024), 3)\n",
    "\n",
    "def analize_format(paths, id):\n",
    "    # read and concat the data (if needed)\n",
    "    # measure the time needed for reading and concating\n",
    "    dfs = []\n",
    "    format = paths[0].split(\".\")[-1]\n",
    "    match format:\n",
    "        case \"parquet\":\n",
    "            dfs = [dd.read_parquet(p) for p in paths]\n",
    "        case \"csv\":\n",
    "            # parse dates separately because pandas backend doesn't support dtype=datetime directly during read_csv :(\n",
    "            datetime_cols = [\"pickup_datetime\", \"dropoff_datetime\"]\n",
    "            corrected_schema = TASK1_NP_SCHEMA.copy()\n",
    "            del corrected_schema[\"pickup_datetime\"]\n",
    "            del corrected_schema[\"dropoff_datetime\"]\n",
    "            dfs = [dd.read_csv(p, dtype=corrected_schema, parse_dates=datetime_cols) for p in paths]\n",
    "        case \"h5\":\n",
    "            dfs = [dd.read_hdf(p, key=\"taxidata\") for p in paths]\n",
    "            # parse dates separately because pandas backend doesn't support dtype=datetime directly during read_hdf :(\n",
    "            for i in range(len(dfs)):\n",
    "                dfs[i][\"pickup_datetime\"] = dd.to_datetime(dfs[i][\"pickup_datetime\"])\n",
    "                dfs[i][\"dropoff_datetime\"] = dd.to_datetime(dfs[i][\"dropoff_datetime\"])\n",
    "    \n",
    "    # timeit read, concat and compute (multiple runs for average and std)\n",
    "    times = []\n",
    "    df = None\n",
    "    for _ in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        df = dd.concat(dfs)\n",
    "        df = df.compute()\n",
    "        end = timeit.default_timer()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    # display size of df in memory\n",
    "    n_rows, n_cols = df.shape\n",
    "    results = {\n",
    "        \"id\": id,\n",
    "        \"format\": format,\n",
    "        \"n_rows\": n_rows,\n",
    "        \"n_cols\": n_cols,\n",
    "        \"mem_usage_GB\": df.memory_usage(deep=True).sum() / (1024 * 1024 * 1024),\n",
    "        \"mean_read_time\": np.mean(times),\n",
    "        \"std_read_time\": np.std(times),\n",
    "        \"size_on_disk_GB\": get_total_size_GB(paths),\n",
    "    }\n",
    "\n",
    "    del df # force cleanup\n",
    "    return results\n",
    "\n",
    "all_original_parquet = list(glob(os.path.join(RAW_DATA_ROOT, \"yellow_tripdata_*.parquet\")))\n",
    "five_years_original_parquet = sum([get_raw_files(RAW_DATA_ROOT, year) for year in range(2020, 2025)], [])\n",
    "one_year_original_parquet = get_raw_files(RAW_DATA_ROOT, 2024)\n",
    "\n",
    "all_parquet = list(glob(os.path.join(TASK1_OUT_ROOT, \"all\", \"*\", \"*.parquet\")))\n",
    "five_years_parquet = list(filter(lambda x: any([os.path.dirname(x).endswith(y) for y in [\"2020\",\"2021\",\"2022\",\"2023\",\"2024\"]]), all_parquet))\n",
    "five_years_csv = os.path.join(TASK1_OUT_ROOT, \"five_years\",\"2020_2024.csv\")\n",
    "one_year_parquet = list(filter(lambda x: os.path.dirname(x).endswith(\"2024\"), all_parquet))\n",
    "one_year_csv = os.path.join(TASK1_OUT_ROOT, \"one_year\",\"2024.csv\")\n",
    "one_year_hdf5 = os.path.join(TASK1_OUT_ROOT, \"one_year\",\"2024.h5\")\n",
    "\n",
    "assert len(all_original_parquet) == 193, f\"Expected 193 original parquet files, but found {len(all_original_parquet)}\"\n",
    "assert len(all_parquet) == 193, f\"Expected 193 parquet files, but found {len(all_parquet)}\"\n",
    "assert len(five_years_original_parquet) == 12*5, f\"Expected 60 parquet files for 5 years, but found {len(five_years_original_parquet)}\"\n",
    "assert len(five_years_parquet) == 12*5\n",
    "assert len(one_year_original_parquet) == 12, f\"Expected 12 parquet files for 1 year, but found {len(one_year_original_parquet)}\"\n",
    "assert len(one_year_parquet) == 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a18eef",
   "metadata": {},
   "source": [
    "### File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ce6c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data sizes:\n",
      "Total size (all years, parquet): 28.941 GB\n",
      "Total size (2020-2024, parquet): 2.606 GB\n",
      "Total size (2024, parquet):      0.645 GB\n",
      "Processed data sizes:\n",
      "Total size (all years, parquet): 35.057 GB\n",
      "Total size (2020-2024, parquet): 3.772 GB\n",
      "Total size (2020-2024, csv):     22.429 GB\n",
      "Total size (2024, parquet):      0.895 GB\n",
      "Total size (2024, csv):          5.305 GB\n",
      "Total size (2024, hdf5):         1.014 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data sizes:\")\n",
    "print(f\"Total size (all years, parquet): {get_total_size_GB(all_original_parquet)} GB\")\n",
    "print(f\"Total size (2020-2024, parquet): {get_total_size_GB(five_years_original_parquet)} GB\")\n",
    "print(f\"Total size (2024, parquet):      {get_total_size_GB(one_year_original_parquet)} GB\")\n",
    "\n",
    "print(\"Processed data sizes:\")\n",
    "print(f\"Total size (all years, parquet): {get_total_size_GB(all_parquet)} GB\")\n",
    "print(f\"Total size (2020-2024, parquet): {get_total_size_GB(five_years_parquet)} GB\")\n",
    "print(f\"Total size (2020-2024, csv):     {get_total_size_GB([five_years_csv])} GB\")\n",
    "print(f\"Total size (2024, parquet):      {get_total_size_GB(one_year_parquet)} GB\")\n",
    "print(f\"Total size (2024, csv):          {get_total_size_GB([one_year_csv])} GB\")\n",
    "print(f\"Total size (2024, hdf5):         {get_total_size_GB([one_year_hdf5])} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d2ecda",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- As expected, the binary formats (parquet and HDF) achieve the smallest file-sizes with parquet pulling ahead.\n",
    "- Our processed datasets are larger than the original parquet ones due to extra columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923ae0b",
   "metadata": {},
   "source": [
    "### Comparing formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b57b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_results = []\n",
    "format_results.append(analize_format(one_year_original_parquet, \"original_1year\"))\n",
    "format_results.append(analize_format(one_year_parquet, \"processed_1year\"))\n",
    "format_results.append(analize_format([one_year_csv], \"processed_1year\"))\n",
    "format_results.append(analize_format([one_year_hdf5], \"processed_1year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f89dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>format</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_cols</th>\n",
       "      <th>mem_usage_GB</th>\n",
       "      <th>mean_read_time</th>\n",
       "      <th>std_read_time</th>\n",
       "      <th>size_on_disk_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original_1year</td>\n",
       "      <td>parquet</td>\n",
       "      <td>41169720</td>\n",
       "      <td>19</td>\n",
       "      <td>5.714</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_1year</td>\n",
       "      <td>parquet</td>\n",
       "      <td>41169720</td>\n",
       "      <td>22</td>\n",
       "      <td>3.297</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_1year</td>\n",
       "      <td>csv</td>\n",
       "      <td>41169720</td>\n",
       "      <td>22</td>\n",
       "      <td>3.336</td>\n",
       "      <td>89.80</td>\n",
       "      <td>1.87</td>\n",
       "      <td>5.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_1year</td>\n",
       "      <td>h5</td>\n",
       "      <td>41169720</td>\n",
       "      <td>22</td>\n",
       "      <td>3.336</td>\n",
       "      <td>16.07</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id   format    n_rows  n_cols  mem_usage_GB  mean_read_time  \\\n",
       "0   original_1year  parquet  41169720      19         5.714            3.32   \n",
       "1  processed_1year  parquet  41169720      22         3.297            4.12   \n",
       "2  processed_1year      csv  41169720      22         3.336           89.80   \n",
       "3  processed_1year       h5  41169720      22         3.336           16.07   \n",
       "\n",
       "   std_read_time  size_on_disk_GB  \n",
       "0           0.83            0.645  \n",
       "1           1.75            0.895  \n",
       "2           1.87            5.305  \n",
       "3           0.32            1.014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "format_results_df = pd.DataFrame(format_results)\n",
    "format_results_df[\"mem_usage_GB\"] = format_results_df[\"mem_usage_GB\"].round(3)\n",
    "format_results_df[\"mean_read_time\"] = format_results_df[\"mean_read_time\"].round(2)\n",
    "format_results_df[\"std_read_time\"] = format_results_df[\"std_read_time\"].round(2)\n",
    "format_results_df[\"size_on_disk_GB\"] = format_results_df[\"size_on_disk_GB\"].round(3)\n",
    "format_results_df.to_csv(os.path.join(RESULTS_ROOT, \"format_analysis.csv\"), index=False)\n",
    "display(format_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ebc8a",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- The original parquet files have wasteful datatypes - our processed datasets have more columns, yet take up 42% less space in memory. The main culprits are the string datatypes (e.g. store_and_forward flag column is a string of either 'Y' or 'N'. We change this to 1 or 0 respectively).\n",
    "- Out of all the formats, parquet is 1 to 2 orders of magnitude faster compared to other formats (CSV being the slowest)\n",
    "- We had trouble using dask's implementation of HDF (dask.dataframe.to_hdf) as it produced larger files than the CSV equivalents. Using a different implementation (h5py) we achieve similar file sizes to parquet.\n",
    "- Bottom-line: for this project we will stick to parquet as it is the fastest to work with, preserves data-types (CSV does not, while HDF allows only ints/floats so datetimes are not parsed automatically). Additionally, plain-text formats like CSV lose the appeal in this case as we cannot inspect them in a text editor due to the sheer size of the file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
