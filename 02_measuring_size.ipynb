{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from constants import TASK1_OUT_ROOT, RAW_DATA_ROOT, TASK1_NP_SCHEMA, RESULTS_ROOT\n",
    "import os\n",
    "from glob import glob\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_raw_files(root_path, year):\n",
    "    return sorted(list(glob(f\"{root_path}/yellow_tripdata_{year}*.parquet\")))\n",
    "\n",
    "def get_total_size_GB(paths):\n",
    "    return round(sum(os.path.getsize(p) for p in paths) / (1024 * 1024 * 1024), 3)\n",
    "\n",
    "def analize_format(paths, id):\n",
    "    # read and concat the data (if needed)\n",
    "    # measure the time needed for reading and concating\n",
    "    dfs = []\n",
    "    format = paths[0].split(\".\")[-1]\n",
    "    match format:\n",
    "        case \"parquet\":\n",
    "            dfs = [dd.read_parquet(p) for p in paths]\n",
    "        case \"csv\":\n",
    "            # parse dates separately because pandas backend doesn't support dtype=datetime directly during read_csv :(\n",
    "            datetime_cols = [\"pickup_datetime\", \"dropoff_datetime\"]\n",
    "            corrected_schema = TASK1_NP_SCHEMA.copy()\n",
    "            del corrected_schema[\"pickup_datetime\"]\n",
    "            del corrected_schema[\"dropoff_datetime\"]\n",
    "            dfs = [dd.read_csv(p, dtype=corrected_schema, parse_dates=datetime_cols) for p in paths]\n",
    "        case \"h5\":\n",
    "            dfs = [dd.read_hdf(p, key=\"taxidata\") for p in paths]\n",
    "            # parse dates separately because pandas backend doesn't support dtype=datetime directly during read_hdf :(\n",
    "            for i in range(len(dfs)):\n",
    "                dfs[i][\"pickup_datetime\"] = dd.to_datetime(dfs[i][\"pickup_datetime\"])\n",
    "                dfs[i][\"dropoff_datetime\"] = dd.to_datetime(dfs[i][\"dropoff_datetime\"])\n",
    "    \n",
    "    # timeit read, concat and compute (multiple runs for average and std)\n",
    "    times = []\n",
    "    df = None\n",
    "    for _ in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        df = dd.concat(dfs)\n",
    "        df = df.compute()\n",
    "        end = timeit.default_timer()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    # display size of df in memory\n",
    "    n_rows, n_cols = df.shape\n",
    "    results = {\n",
    "        \"id\": id,\n",
    "        \"format\": format,\n",
    "        \"n_rows\": n_rows,\n",
    "        \"n_cols\": n_cols,\n",
    "        \"mem_usage_GB\": df.memory_usage(deep=True).sum() / (1024 * 1024 * 1024),\n",
    "        \"mean_read_time\": np.mean(times),\n",
    "        \"std_read_time\": np.std(times),\n",
    "        \"size_on_disk_GB\": get_total_size_GB(paths),\n",
    "    }\n",
    "\n",
    "    del df # force cleanup\n",
    "    return results\n",
    "\n",
    "all_original_parquet = list(glob(os.path.join(RAW_DATA_ROOT, \"yellow_tripdata_*.parquet\")))\n",
    "five_years_original_parquet = sum([get_raw_files(RAW_DATA_ROOT, year) for year in range(2020, 2025)], [])\n",
    "one_year_original_parquet = get_raw_files(RAW_DATA_ROOT, 2024)\n",
    "\n",
    "all_parquet = list(glob(os.path.join(TASK1_OUT_ROOT, \"all\", \"*\", \"*.parquet\")))\n",
    "five_years_parquet = list(filter(lambda x: any([os.path.dirname(x).endswith(y) for y in [\"2020\",\"2021\",\"2022\",\"2023\",\"2024\"]]), all_parquet))\n",
    "five_years_csv = os.path.join(TASK1_OUT_ROOT, \"five_years\",\"2020_2024.csv\")\n",
    "one_year_parquet = list(filter(lambda x: os.path.dirname(x).endswith(\"2024\"), all_parquet))\n",
    "one_year_csv = os.path.join(TASK1_OUT_ROOT, \"one_year\",\"2024.csv\")\n",
    "one_year_hdf5 = os.path.join(TASK1_OUT_ROOT, \"one_year\",\"2024.h5\")\n",
    "\n",
    "assert len(all_original_parquet) == 193, f\"Expected 193 original parquet files, but found {len(all_original_parquet)}\"\n",
    "assert len(all_parquet) == 193, f\"Expected 193 parquet files, but found {len(all_parquet)}\"\n",
    "assert len(five_years_original_parquet) == 12*5, f\"Expected 60 parquet files for 5 years, but found {len(five_years_original_parquet)}\"\n",
    "assert len(five_years_parquet) == 12*5\n",
    "assert len(one_year_original_parquet) == 12, f\"Expected 12 parquet files for 1 year, but found {len(one_year_original_parquet)}\"\n",
    "assert len(one_year_parquet) == 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a18eef",
   "metadata": {},
   "source": [
    "### File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ce6c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data sizes:\n",
      "Total size (all years, parquet): 28.941 GB\n",
      "Total size (2020-2024, parquet): 2.606 GB\n",
      "Total size (2024, parquet):      0.645 GB\n",
      "Processed data sizes:\n",
      "Total size (all years, parquet): 35.057 GB\n",
      "Total size (2020-2024, parquet): 3.772 GB\n",
      "Total size (2024, parquet):      0.895 GB\n",
      "Total size (2024, csv):          5.305 GB\n",
      "Total size (2024, hdf5):         1.014 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data sizes:\")\n",
    "print(f\"Total size (all years, parquet): {get_total_size_GB(all_original_parquet)} GB\")\n",
    "print(f\"Total size (2020-2024, parquet): {get_total_size_GB(five_years_original_parquet)} GB\")\n",
    "print(f\"Total size (2024, parquet):      {get_total_size_GB(one_year_original_parquet)} GB\")\n",
    "\n",
    "print(\"Processed data sizes:\")\n",
    "print(f\"Total size (all years, parquet): {get_total_size_GB(all_parquet)} GB\")\n",
    "print(f\"Total size (2020-2024, parquet): {get_total_size_GB(five_years_parquet)} GB\")\n",
    "# print(f\"Total size (2020-2024, csv):     {get_total_size_GB([five_years_csv])} GB\")\n",
    "print(f\"Total size (2024, parquet):      {get_total_size_GB(one_year_parquet)} GB\")\n",
    "print(f\"Total size (2024, csv):          {get_total_size_GB([one_year_csv])} GB\")\n",
    "print(f\"Total size (2024, hdf5):         {get_total_size_GB([one_year_hdf5])} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923ae0b",
   "metadata": {},
   "source": [
    "### Comparing formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b57b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_results = []\n",
    "# format_results.append(analize_format(all_original_parquet, \"original_all\"))\n",
    "# format_results.append(analize_format(all_parquet, \"processed_all\"))\n",
    "# format_results.append(analize_format(five_years_original_parquet, \"original_5years\"))\n",
    "# format_results.append(analize_format(five_years_parquet, \"processed_5years\"))\n",
    "# format_results.append(analize_format([five_years_csv], \"processed_5years\"))\n",
    "format_results.append(analize_format(one_year_original_parquet, \"original_1year\"))\n",
    "format_results.append(analize_format(one_year_parquet, \"processed_1year\"))\n",
    "format_results.append(analize_format([one_year_csv], \"processed_1year\"))\n",
    "format_results.append(analize_format([one_year_hdf5], \"processed_1year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f89dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>format</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_cols</th>\n",
       "      <th>mem_usage_GB</th>\n",
       "      <th>mean_read_time</th>\n",
       "      <th>std_read_time</th>\n",
       "      <th>size_on_disk_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original_1year</td>\n",
       "      <td>parquet</td>\n",
       "      <td>41169720</td>\n",
       "      <td>19</td>\n",
       "      <td>5.714</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_1year</td>\n",
       "      <td>parquet</td>\n",
       "      <td>41169720</td>\n",
       "      <td>22</td>\n",
       "      <td>3.297</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_1year</td>\n",
       "      <td>csv</td>\n",
       "      <td>41169720</td>\n",
       "      <td>22</td>\n",
       "      <td>3.336</td>\n",
       "      <td>88.57</td>\n",
       "      <td>1.10</td>\n",
       "      <td>5.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_1year</td>\n",
       "      <td>h5</td>\n",
       "      <td>41169720</td>\n",
       "      <td>22</td>\n",
       "      <td>3.336</td>\n",
       "      <td>16.02</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id   format    n_rows  n_cols  mem_usage_GB  mean_read_time  \\\n",
       "0   original_1year  parquet  41169720      19         5.714            2.61   \n",
       "1  processed_1year  parquet  41169720      22         3.297            2.52   \n",
       "2  processed_1year      csv  41169720      22         3.336           88.57   \n",
       "3  processed_1year       h5  41169720      22         3.336           16.02   \n",
       "\n",
       "   std_read_time  size_on_disk_GB  \n",
       "0           0.06            0.645  \n",
       "1           0.06            0.895  \n",
       "2           1.10            5.305  \n",
       "3           0.19            1.014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "format_results_df = pd.DataFrame(format_results)\n",
    "format_results_df[\"mem_usage_GB\"] = format_results_df[\"mem_usage_GB\"].round(3)\n",
    "format_results_df[\"mean_read_time\"] = format_results_df[\"mean_read_time\"].round(2)\n",
    "format_results_df[\"std_read_time\"] = format_results_df[\"std_read_time\"].round(2)\n",
    "format_results_df[\"size_on_disk_GB\"] = format_results_df[\"size_on_disk_GB\"].round(3)\n",
    "format_results_df.to_csv(os.path.join(RESULTS_ROOT, \"format_analysis.csv\"), index=False)\n",
    "display(format_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
