{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4611154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t7_service_classifier_pandas_chrono_featimp.py\n",
    "import os\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from utils.constants import (\n",
    "    TASK2_OUT_ROOT,\n",
    "    TASK7_FHV_SCHEMA,\n",
    "    TASK7_FHVHV_SCHEMA,\n",
    "    TASK7_GREENTAXI_SCHEMA,\n",
    "    TASK7_YELLOWTAXI_SCHEMA,\n",
    "    LATEX_ROOT,\n",
    "    RESULTS_ROOT,\n",
    ")\n",
    "\n",
    "# -------------------- STYLE / IO --------------------\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "FIG_DIR = os.path.join(LATEX_ROOT, \"figures\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "SERVICE_DIR = {\n",
    "    \"Yellow\": \"yellow_tripdata\",\n",
    "    \"Green\":  \"green_tripdata\",\n",
    "    \"FHV\":    \"fhv_tripdata\",\n",
    "    \"FHVHV\":  \"fhvhv_tripdata\",\n",
    "}\n",
    "SCHEMA_MAP: Dict[str, Dict[str, Any]] = {\n",
    "    \"Yellow\": TASK7_YELLOWTAXI_SCHEMA,\n",
    "    \"Green\":  TASK7_GREENTAXI_SCHEMA,\n",
    "    \"FHV\":    TASK7_FHV_SCHEMA,\n",
    "    \"FHVHV\":  TASK7_FHVHV_SCHEMA,\n",
    "}\n",
    "\n",
    "# union of columns we may want (read only those that exist)\n",
    "DESIRED = [\n",
    "    \"pickup_datetime\", \"dropoff_datetime\",\n",
    "    \"pickup_borough\", \"dropoff_borough\",\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\", \"tip_amount\",            # Yellow/Green\n",
    "    \"base_passenger_fare\", \"tips\",          # FHVHV\n",
    "]\n",
    "\n",
    "# two consecutive months → chronological split\n",
    "MONTHS = [(2023, 3), (2023, 4)]    # earlier = train, later = test\n",
    "SERVICES = [\"Yellow\", \"Green\", \"FHV\", \"FHVHV\"]\n",
    "MAX_ROWS = 200_000                 # cap per service per month (None = all)\n",
    "\n",
    "TOP_K = 25                         # features to plot\n",
    "PERMUTATION_SAMPLE = 20_000        # rows for permutation fallback\n",
    "PERMUTATION_REPEATS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def _astype_with_schema(df: pd.DataFrame, schema: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Cast columns present in df to types from schema; coerce datetimes.\"\"\"\n",
    "    for col, dt in schema.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        try:\n",
    "            df[col] = df[col].astype(dt)\n",
    "        except Exception:\n",
    "            if isinstance(dt, str) and dt.startswith(\"datetime64\"):\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def read_service_month(service: str, year: int, month: int, max_rows: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Read a single month with predicate pushdown; unify monetary columns.\"\"\"\n",
    "    year_dir = os.path.join(TASK2_OUT_ROOT, SERVICE_DIR[service], f\"year={year}\")\n",
    "    if not os.path.isdir(year_dir):\n",
    "        print(f\"[WARN] missing: {year_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    avail = set(ds.dataset(year_dir, format=\"parquet\").schema.names)\n",
    "    cols = [c for c in DESIRED if c in avail]\n",
    "\n",
    "    start = pd.Timestamp(year=year, month=month, day=1)\n",
    "    end   = start + pd.offsets.MonthBegin(1)\n",
    "\n",
    "    df = pd.read_parquet(\n",
    "        year_dir,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=cols,\n",
    "        filters=[(\"pickup_datetime\", \">=\", start),\n",
    "                 (\"pickup_datetime\", \"<\",  end)],\n",
    "    )\n",
    "\n",
    "    df = _astype_with_schema(df, SCHEMA_MAP[service])\n",
    "\n",
    "    # unify monetary names\n",
    "    if service in (\"Yellow\", \"Green\"):\n",
    "        df[\"fare\"] = df[\"fare_amount\"] if \"fare_amount\" in df.columns else np.nan\n",
    "        df[\"tip\"]  = df[\"tip_amount\"]  if \"tip_amount\"  in df.columns else np.nan\n",
    "    elif service == \"FHVHV\":\n",
    "        df[\"fare\"] = df[\"base_passenger_fare\"] if \"base_passenger_fare\" in df.columns else np.nan\n",
    "        df[\"tip\"]  = df[\"tips\"]               if \"tips\"               in df.columns else np.nan\n",
    "    else:  # FHV typically has no monetary fields\n",
    "        df[\"fare\"] = np.nan\n",
    "        df[\"tip\"]  = np.nan\n",
    "\n",
    "    df[\"service\"] = service\n",
    "\n",
    "    if max_rows and len(df) > max_rows:\n",
    "        df = df.sample(n=max_rows, random_state=RANDOM_STATE)\n",
    "\n",
    "    keep = [\n",
    "        \"pickup_datetime\",\"dropoff_datetime\",\n",
    "        \"pickup_borough\",\"dropoff_borough\",\n",
    "        \"trip_distance\",\"fare\",\"tip\",\"service\",\n",
    "    ]\n",
    "    keep = [c for c in keep if c in df.columns]\n",
    "    return df[keep].reset_index(drop=True)\n",
    "\n",
    "def add_features(dfx: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Feature engineering inplace; returns the same frame.\"\"\"\n",
    "    # duration & speed\n",
    "    dur_min = (dfx[\"dropoff_datetime\"] - dfx[\"pickup_datetime\"]).dt.total_seconds() / 60.0\n",
    "    dur_min = dur_min.clip(lower=0.1, upper=6*60)   # guard\n",
    "    dfx[\"duration_min\"] = dur_min\n",
    "    dur_hr = dur_min / 60.0\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        speed = dfx.get(\"trip_distance\", pd.Series(np.nan, index=dfx.index)) / dur_hr\n",
    "    dfx[\"speed_mph\"] = np.where(np.isfinite(speed), speed, np.nan).clip(0, 120)\n",
    "\n",
    "    # calendar\n",
    "    dfx[\"hour\"] = dfx[\"pickup_datetime\"].dt.hour.astype(\"int16\")\n",
    "    dfx[\"dow\"]  = dfx[\"pickup_datetime\"].dt.dayofweek.astype(\"int16\")\n",
    "\n",
    "    # categories\n",
    "    dfx[\"pickup_borough\"]  = dfx[\"pickup_borough\"].astype(\"string\")\n",
    "    dfx[\"dropoff_borough\"] = dfx[\"dropoff_borough\"].astype(\"string\")\n",
    "    dfx[\"od_pair\"] = (dfx[\"pickup_borough\"].fillna(\"NA\") + \"→\" +\n",
    "                      dfx[\"dropoff_borough\"].fillna(\"NA\")).astype(\"string\")\n",
    "\n",
    "    # money-derived\n",
    "    dfx[\"fare\"] = dfx.get(\"fare\", np.nan).astype(\"float32\")\n",
    "    dfx[\"tip\"]  = dfx.get(\"tip\",  np.nan).astype(\"float32\")\n",
    "    dfx[\"has_fare\"] = dfx[\"fare\"].notna().astype(\"int8\")\n",
    "    dfx[\"has_tip\"]  = (dfx[\"tip\"].notna() & (dfx[\"tip\"] > 0)).astype(\"int8\")\n",
    "    dfx[\"tip_rate\"] = np.where(dfx[\"fare\"] > 0, dfx[\"tip\"] / dfx[\"fare\"], np.nan).astype(\"float32\")\n",
    "    dfx[\"log_fare\"] = np.where(dfx[\"fare\"] > 0, np.log1p(dfx[\"fare\"]), np.nan).astype(\"float32\")\n",
    "    return dfx\n",
    "\n",
    "def OneHotDense(**kwargs):\n",
    "    \"\"\"Compat wrapper around OneHotEncoder sparse_output/sparse param across sklearn versions.\"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(sparse_output=False, **kwargs)  # sklearn ≥ 1.2\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(sparse=False, **kwargs)\n",
    "\n",
    "def pretty_names(names: np.ndarray) -> List[str]:\n",
    "    out = []\n",
    "    for n in names:\n",
    "        s = str(n)\n",
    "        s = s.replace(\"num__\", \"\").replace(\"cat__\", \"\")\n",
    "        s = s.replace(\"pickup_borough_\", \"PU:\")\n",
    "        s = s.replace(\"dropoff_borough_\", \"DO:\")\n",
    "        s = s.replace(\"od_pair_\", \"\")\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "# -------------------- LOAD TWO MONTHS --------------------\n",
    "frames = []\n",
    "for (Y, M) in MONTHS:\n",
    "    for svc in SERVICES:\n",
    "        part = read_service_month(svc, Y, M, max_rows=MAX_ROWS)\n",
    "        print(f\"{svc} {Y}-{M:02d}: {len(part):,} rows\")\n",
    "        if not part.empty:\n",
    "            part[\"yy_mm\"] = f\"{Y}-{M:02d}\"\n",
    "            frames.append(part)\n",
    "\n",
    "if not frames:\n",
    "    raise SystemExit(\"No data loaded. Check paths/months.\")\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True).sort_values(\"pickup_datetime\").reset_index(drop=True)\n",
    "\n",
    "# -------------------- SPLIT, FEATURES --------------------\n",
    "last_year, last_month = MONTHS[-1]\n",
    "split_ts = pd.Timestamp(year=last_year, month=last_month, day=1)\n",
    "train = df[df[\"pickup_datetime\"] < split_ts].copy()\n",
    "test  = df[df[\"pickup_datetime\"] >= split_ts].copy()\n",
    "\n",
    "print(f\"\\nSplit at {split_ts}\")\n",
    "print(f\"Train rows: {len(train):,} | Test rows: {len(test):,}\")\n",
    "print(\"\\nClass counts (train):\\n\", train[\"service\"].value_counts())\n",
    "print(\"\\nClass counts (test):\\n\",  test[\"service\"].value_counts())\n",
    "\n",
    "train = add_features(train)\n",
    "test  = add_features(test)\n",
    "\n",
    "y_train = train[\"service\"]\n",
    "y_test  = test[\"service\"]\n",
    "\n",
    "# -------------------- PREPROCESSOR --------------------\n",
    "# Use only behavior-level features (no monetary fields)\n",
    "num_cols = [\"trip_distance\", \"duration_min\", \"speed_mph\", \"hour\", \"dow\"]\n",
    "cat_cols = [\"pickup_borough\", \"dropoff_borough\", \"od_pair\"]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), [c for c in num_cols if c in train.columns]),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=100),\n",
    "         [c for c in cat_cols if c in train.columns]),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), [c for c in num_cols if c in train.columns]),\n",
    "        (\"cat\", OneHotDense(handle_unknown=\"ignore\", min_frequency=100),\n",
    "         [c for c in cat_cols if c in train.columns]),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=True,\n",
    ")\n",
    "\n",
    "# -------------------- MODELS --------------------\n",
    "models = {\n",
    "    \"LogReg_balanced\": LogisticRegression(max_iter=300, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "    \"HGB\": HistGradientBoostingClassifier(\n",
    "        max_iter=300, learning_rate=0.1,\n",
    "        early_stopping=True, validation_fraction=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "}\n",
    "\n",
    "labels = sorted(pd.concat([y_train, y_test]).unique())\n",
    "results = []\n",
    "\n",
    "# -------------------- TRAIN / EVAL / FEATURE IMPORTANCE --------------------\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    pipe.fit(train, y_train)\n",
    "    pred = pipe.predict(test)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    f1m = f1_score(y_test, pred, average=\"macro\")\n",
    "\n",
    "    print(f\"\\n=== {name} (chronological split) ===\")\n",
    "    print(f\"Accuracy: {acc:.3f} | Macro-F1: {f1m:.3f}\")\n",
    "    print(classification_report(y_test, pred, digits=3))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, pred, labels=labels)\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Service classifier — {name} (chronological split)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_DIR, f\"t7_cm_{name}_chrono.pdf\"))\n",
    "    plt.savefig(os.path.join(FIG_DIR, f\"t7_cm_{name}_chrono.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    # ---------- Feature names from the ColumnTransformer ----------\n",
    "    feat_names = pipe.named_steps[\"prep\"].get_feature_names_out()\n",
    "    feat_names = np.array(pretty_names(feat_names))\n",
    "\n",
    "    # ---------- Importances ----------\n",
    "    importances = None\n",
    "\n",
    "    if name.startswith(\"LogReg\"):\n",
    "        # Use L2 norm across classes for multinomial LR\n",
    "        coefs = pipe.named_steps[\"clf\"].coef_\n",
    "        importances = np.linalg.norm(coefs, axis=0)\n",
    "\n",
    "    elif name == \"HGB\":\n",
    "        hgb = pipe.named_steps[\"clf\"]\n",
    "        if hasattr(hgb, \"feature_importances_\"):\n",
    "            importances = hgb.feature_importances_\n",
    "        else:\n",
    "            # Fallback: permutation importance on transformed *test* sample\n",
    "            prep = pipe.named_steps[\"prep\"]\n",
    "            cols_in = getattr(prep, \"feature_names_in_\", None)\n",
    "            if cols_in is None:\n",
    "                cols_in = test.columns  # last resort\n",
    "\n",
    "            per_class = max(1, PERMUTATION_SAMPLE // max(1, len(labels)))\n",
    "            sample_df = (\n",
    "                test.groupby(\"service\", group_keys=False)\n",
    "                    .apply(lambda d: d.sample(min(len(d), per_class), random_state=RANDOM_STATE))\n",
    "            )\n",
    "            Xt = prep.transform(sample_df.loc[:, cols_in])\n",
    "            yt = sample_df[\"service\"].values\n",
    "\n",
    "            perm = permutation_importance(\n",
    "                hgb, Xt, yt,\n",
    "                n_repeats=PERMUTATION_REPEATS,\n",
    "                random_state=RANDOM_STATE,\n",
    "                scoring=\"accuracy\",\n",
    "            )\n",
    "            importances = perm.importances_mean\n",
    "\n",
    "    # Plot & save top-K importances (if computed)\n",
    "    if importances is not None:\n",
    "        imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
    "        imp_df = imp_df.sort_values(\"importance\", ascending=False).head(TOP_K)\n",
    "\n",
    "        imp_csv = os.path.join(RESULTS_ROOT, f\"t7_feature_importance_{name}.csv\")\n",
    "        imp_df.to_csv(imp_csv, index=False)\n",
    "\n",
    "        plt.figure(figsize=(8, max(4, 0.35 * len(imp_df))))\n",
    "        sns.barplot(\n",
    "            data=imp_df.sort_values(\"importance\", ascending=True),\n",
    "            x=\"importance\", y=\"feature\", orient=\"h\"\n",
    "        )\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(f\"Top-{TOP_K} feature importances — {name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIG_DIR, f\"t7_featimp_{name}.pdf\"))\n",
    "        plt.savefig(os.path.join(FIG_DIR, f\"t7_featimp_{name}.png\"), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "    results.append({\"model\": name, \"accuracy\": acc, \"macro_f1\": f1m})\n",
    "\n",
    "# Save summary\n",
    "res_df = pd.DataFrame(results).sort_values(\"macro_f1\", ascending=False)\n",
    "res_df.to_csv(os.path.join(RESULTS_ROOT, \"t7_service_classifier_chrono_results.csv\"), index=False)\n",
    "print(\"\\nSaved results, confusion matrices, and feature-importance plots.\")\n",
    "print(\"[OK] Figures →\", FIG_DIR)\n",
    "print(\"[OK] CSVs    →\", RESULTS_ROOT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd-project-25 (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
