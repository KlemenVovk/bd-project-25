{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4611154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen per-class rows (balanced train batches): {'FHVHV': 110006, 'Yellow': 110006, 'Green': 110006}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:53,  4.10s/it]\n",
      "13it [00:59,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training row estimates (metadata): {'Yellow': 36475743, 'Green': 708386, 'FHVHV': 220664250}\n",
      "Majority baseline (train): FHVHV\n",
      "\n",
      "Majority baseline: macro-accuracy=0.333, macro-F1=0.250 (raw acc=0.600)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Yellow      0.000     0.000     0.000    120024\n",
      "       Green      0.000     0.000     0.000    120004\n",
      "       FHVHV      0.600     1.000     0.750    360000\n",
      "\n",
      "    accuracy                          0.600    600028\n",
      "   macro avg      0.200     0.333     0.250    600028\n",
      "weighted avg      0.360     0.600     0.450    600028\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/home/kvovk/work/bd-project-25/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/var/home/kvovk/work/bd-project-25/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/var/home/kvovk/work/bd-project-25/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD (partial_fit, balanced train): macro-accuracy=0.552, macro-F1=0.484 (raw acc=0.542)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Yellow      0.371     0.883     0.523    120024\n",
      "       Green      0.313     0.246     0.275    120004\n",
      "       FHVHV      0.860     0.527     0.653    360000\n",
      "\n",
      "    accuracy                          0.542    600028\n",
      "   macro avg      0.515     0.552     0.484    600028\n",
      "weighted avg      0.653     0.542     0.551    600028\n",
      "\n",
      "\n",
      "XGBoost (incremental, balanced train): macro-accuracy=0.559, macro-F1=0.448 (raw acc=0.449)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Yellow      0.367     0.818     0.507    120024\n",
      "       Green      0.310     0.574     0.402    120004\n",
      "       FHVHV      0.927     0.284     0.435    360000\n",
      "\n",
      "    accuracy                          0.449    600028\n",
      "   macro avg      0.535     0.559     0.448    600028\n",
      "weighted avg      0.691     0.449     0.443    600028\n",
      "\n",
      "\n",
      "[OK] Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple, Iterable, Optional\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,            # kept for console info if you want it\n",
    "    balanced_accuracy_score,   # <-- macro avg accuracy\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from utils.constants import TASK2_OUT_ROOT, LATEX_ROOT, RESULTS_ROOT\n",
    "\n",
    "FIG_DIR = os.path.join(LATEX_ROOT, \"figures\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
    "\n",
    "SERVICE_DIR = {\n",
    "    \"Yellow\": \"yellow_tripdata\",\n",
    "    \"Green\":  \"green_tripdata\",\n",
    "    \"FHV\":    \"fhv_tripdata\",\n",
    "    \"FHVHV\":  \"fhvhv_tripdata\",\n",
    "}\n",
    "\n",
    "# classes used (FHV excluded: often lacks trip_distance)\n",
    "CLASSES = [\"Yellow\", \"Green\", \"FHVHV\"]\n",
    "CLASS2ID = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "# features\n",
    "NUM_COLS = [\"trip_distance\", \"duration_min\", \"speed_mph\", \"hour\", \"dow\"]\n",
    "CAT_COLS = [\"pickup_borough\", \"dropoff_borough\"]  # od_pair removed\n",
    "N_HASH = 2**15\n",
    "HASH_ALT_SIGN = False\n",
    "HASHED_LABEL = \"Borough categories (hashed total)\"   # <-- nicer label\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# streaming-friendly transformers\n",
    "scaler = StandardScaler(with_mean=True)  # fit once, then transform batches\n",
    "hasher = FeatureHasher(n_features=N_HASH, input_type=\"string\",\n",
    "                       alternate_sign=HASH_ALT_SIGN)\n",
    "\n",
    "# SGD config\n",
    "SGD_KW = dict(\n",
    "    loss=\"log_loss\",\n",
    "    penalty=\"elasticnet\", l1_ratio=0.15,\n",
    "    learning_rate=\"constant\", eta0=0.03,\n",
    "    average=True,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "N_EPOCHS = 2\n",
    "\n",
    "# XGBoost config (incremental boosting across batches)\n",
    "XGB_PARAMS = dict(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(CLASSES),\n",
    "    tree_method=os.environ.get(\"XGB_TREE\", \"hist\"),   # set to \"gpu_hist\" if you have a GPU\n",
    "    max_depth=6,\n",
    "    eta=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    seed=RANDOM_STATE,\n",
    ")\n",
    "XGB_ROUNDS_PER_BATCH = 50\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _present_columns(fp: str) -> List[str]:\n",
    "    try:\n",
    "        return pq.read_schema(fp).names\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _read_part(fp: str, service: str, cap: Optional[int] = None) -> pd.DataFrame:\n",
    "    want = [\"pickup_datetime\",\"dropoff_datetime\",\"pickup_borough\",\"dropoff_borough\",\"trip_distance\"]\n",
    "    avail = set(_present_columns(fp))\n",
    "    cols = [c for c in want if c in avail]\n",
    "    if not cols:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_parquet(fp, engine=\"pyarrow\", columns=cols)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    if cap and len(df) > cap:\n",
    "        df = df.sample(n=cap, random_state=RANDOM_STATE)\n",
    "    df[\"service\"] = service\n",
    "    return df\n",
    "\n",
    "def _engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in [\"pickup_datetime\",\"dropoff_datetime\",\"pickup_borough\",\"dropoff_borough\",\"trip_distance\",\"service\"]:\n",
    "        if c not in df.columns: df[c] = np.nan\n",
    "\n",
    "    dt_pu = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "    dt_do = pd.to_datetime(df[\"dropoff_datetime\"])\n",
    "    dur_min = (dt_do - dt_pu).dt.total_seconds() / 60.0\n",
    "    df[\"duration_min\"] = np.clip(dur_min, 0.1, 6*60)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        speed = df[\"trip_distance\"].astype(\"float32\").to_numpy() / np.where(df[\"duration_min\"] > 0, df[\"duration_min\"]/60.0, np.nan)\n",
    "    df[\"speed_mph\"] = np.clip(np.where(np.isfinite(speed), speed, np.nan), 0, 120)\n",
    "\n",
    "    df[\"hour\"] = dt_pu.dt.hour.astype(\"Int16\")\n",
    "    df[\"dow\"]  = dt_pu.dt.dayofweek.astype(\"Int16\")\n",
    "\n",
    "    df[\"pickup_borough\"]  = df[\"pickup_borough\"].astype(\"string\").fillna(\"NA\")\n",
    "    df[\"dropoff_borough\"] = df[\"dropoff_borough\"].astype(\"string\").fillna(\"NA\")\n",
    "\n",
    "    return df[[\"service\"] + NUM_COLS + CAT_COLS]\n",
    "\n",
    "def _design(df: pd.DataFrame) -> Tuple[sparse.csr_matrix, np.ndarray]:\n",
    "    # numeric\n",
    "    Xn = df[NUM_COLS].astype(\"float32\").replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    Xn = scaler.transform(Xn).astype(\"float32\")\n",
    "    Xn = sparse.csr_matrix(Xn)\n",
    "    # hashed categoricals (PU/DO only)\n",
    "    tokens = [[f\"PU:{a}\", f\"DO:{b}\"] for a,b in df[CAT_COLS].itertuples(index=False)]\n",
    "    Xc = hasher.transform(tokens).tocsr().astype(\"float32\")\n",
    "    # stack\n",
    "    X = sparse.hstack([Xn, Xc], format=\"csr\")\n",
    "    y = df[\"service\"].map(CLASS2ID).to_numpy(dtype=np.int32, na_value=-1)\n",
    "    return X, y\n",
    "\n",
    "# ---------- exact-balanced training batches (caps optional) ----------\n",
    "def balanced_batches(files_by_service: Dict[str, List[str]],\n",
    "                     cap_per_file: Optional[Dict[str, int]] = None) -> Iterable[pd.DataFrame]:\n",
    "    services = list(files_by_service.keys())\n",
    "    max_len = min([len(files_by_service.get(s, [])) for s in services]) if services else 0\n",
    "    cap_per_file = cap_per_file or {}\n",
    "    for i in range(max_len):\n",
    "        per_service = {}\n",
    "        for svc in services:\n",
    "            fp = files_by_service[svc][i]\n",
    "            df = _read_part(fp, svc, cap=cap_per_file.get(svc))\n",
    "            df = _engineer(df)\n",
    "            if df.empty:\n",
    "                per_service = {}\n",
    "                break\n",
    "            per_service[svc] = df\n",
    "        if len(per_service) != len(services):\n",
    "            continue\n",
    "        n_min = min(len(d) for d in per_service.values())\n",
    "        if n_min == 0:\n",
    "            continue\n",
    "        balanced_parts = [\n",
    "            d.sample(n=n_min, random_state=RANDOM_STATE) if len(d) > n_min else d\n",
    "            for d in per_service.values()\n",
    "        ]\n",
    "        batch = pd.concat(balanced_parts, ignore_index=True)\n",
    "        batch = batch.sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "        yield batch\n",
    "\n",
    "# ---------- unbalanced validation streaming (caps optional) ----------\n",
    "def stream_validation(files_by_service: Dict[str, List[str]],\n",
    "                      cap_per_file: Optional[Dict[str, int]] = None) -> Iterable[pd.DataFrame]:\n",
    "    cap_per_file = cap_per_file or {}\n",
    "    for svc, files in files_by_service.items():\n",
    "        for fp in files:\n",
    "            df = _engineer(_read_part(fp, svc, cap=cap_per_file.get(svc)))\n",
    "            if not df.empty:\n",
    "                yield df\n",
    "\n",
    "# ---------- majority class from parquet metadata ----------\n",
    "def majority_class_from_files(train_files: Dict[str, List[str]]) -> str:\n",
    "    counts = {}\n",
    "    for svc, files in train_files.items():\n",
    "        total = 0\n",
    "        for fp in files:\n",
    "            try:\n",
    "                total += pq.ParquetFile(fp).metadata.num_rows\n",
    "            except Exception:\n",
    "                pass\n",
    "        counts[svc] = total\n",
    "    maj = max(counts.items(), key=lambda kv: kv[1])[0] if counts else CLASSES[-1]\n",
    "    print(\"Training row estimates (metadata):\", counts)\n",
    "    print(\"Majority baseline (train):\", maj)\n",
    "    return maj\n",
    "\n",
    "# ---------- training / evaluation ----------\n",
    "def train_and_eval(\n",
    "    train_files: Dict[str, List[str]],\n",
    "    valid_files: Dict[str, List[str]],\n",
    "    cap_per_file: Optional[Dict[str, int]] = None,\n",
    "):\n",
    "    # Pass 1: fit scaler on balanced batches\n",
    "    seen = Counter()\n",
    "    for batch in balanced_batches(train_files, cap_per_file=cap_per_file):\n",
    "        Xn = batch[NUM_COLS].astype(\"float32\").replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        scaler.partial_fit(Xn)\n",
    "        seen.update(batch[\"service\"].tolist())\n",
    "    print(\"Seen per-class rows (balanced train batches):\", dict(seen))\n",
    "\n",
    "    # Pass 2: train SGD and XGB together\n",
    "    clf = SGDClassifier(**SGD_KW)\n",
    "    inited = False\n",
    "    booster = None\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        for batch in tqdm(balanced_batches(train_files, cap_per_file=cap_per_file)):\n",
    "            Xb, yb = _design(batch)\n",
    "            m = (yb >= 0)\n",
    "            if not np.any(m):\n",
    "                continue\n",
    "            Xb, yb = Xb[m], yb[m]\n",
    "\n",
    "            # SGD\n",
    "            if not inited:\n",
    "                clf.partial_fit(Xb, yb, classes=np.arange(len(CLASSES)))\n",
    "                inited = True\n",
    "            else:\n",
    "                clf.partial_fit(Xb, yb)\n",
    "\n",
    "            # XGBoost — add trees per batch\n",
    "            dtrain = xgb.DMatrix(Xb, label=yb)\n",
    "            booster = xgb.train(\n",
    "                params=XGB_PARAMS,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=XGB_ROUNDS_PER_BATCH,\n",
    "                xgb_model=booster,\n",
    "            )\n",
    "\n",
    "    # --------- EVAL on unbalanced validation ---------\n",
    "    y_true_all, y_pred_sgd_all, y_pred_xgb_all = [], [], []\n",
    "    for dfv in stream_validation(valid_files, cap_per_file=cap_per_file):\n",
    "        Xv, yv = _design(dfv)\n",
    "        m = (yv >= 0)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        Xv, yv = Xv[m], yv[m]\n",
    "        y_pred_sgd_all.append(clf.predict(Xv))\n",
    "        y_pred_xgb_all.append(booster.predict(xgb.DMatrix(Xv)).argmax(axis=1))\n",
    "        y_true_all.append(yv)\n",
    "\n",
    "    if not y_true_all:\n",
    "        print(\"No validation data — check file lists.\")\n",
    "        return clf, booster\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_sgd  = np.concatenate(y_pred_sgd_all)\n",
    "    y_xgb  = np.concatenate(y_pred_xgb_all)\n",
    "\n",
    "    # Majority baseline (ignores caps for a strong baseline)\n",
    "    maj_label = majority_class_from_files(train_files)\n",
    "    maj_id = CLASS2ID[maj_label]\n",
    "    y_maj = np.full_like(y_true, maj_id)\n",
    "\n",
    "    def summarize(name, yhat, fig_suffix):\n",
    "        # macro-averaged accuracy = balanced accuracy\n",
    "        macro_acc = balanced_accuracy_score(y_true, yhat)\n",
    "        macro_f1  = f1_score(y_true, yhat, average=\"macro\")\n",
    "        acc_raw   = accuracy_score(y_true, yhat)  # optional console info\n",
    "\n",
    "        print(f\"\\n{name}: macro-accuracy={macro_acc:.3f}, macro-F1={macro_f1:.3f} (raw acc={acc_raw:.3f})\")\n",
    "        print(classification_report(y_true, yhat, target_names=CLASSES, digits=3))\n",
    "\n",
    "        # normalized confusion matrix (row-normalized), .2f annotations, PDF only\n",
    "        cm_norm = confusion_matrix(y_true, yhat,\n",
    "                                   labels=np.arange(len(CLASSES)),\n",
    "                                   normalize=\"true\")\n",
    "        cm_df = pd.DataFrame(cm_norm,\n",
    "                             index=[f\"true_{c}\" for c in CLASSES],\n",
    "                             columns=[f\"pred_{c}\" for c in CLASSES])\n",
    "        plt.figure(figsize=(6.4, 5.2))\n",
    "        sns.heatmap(cm_df, annot=True, fmt=\".2f\", cmap=\"Blues\", vmin=0.0, vmax=1.0)\n",
    "        plt.title(f\"Service classifier — {name} (normalized confusion)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIG_DIR, f\"t7_cm_{fig_suffix}.pdf\"))\n",
    "        plt.close()\n",
    "        return macro_acc, macro_f1\n",
    "\n",
    "    acc_maj, f1_maj = summarize(\"Majority baseline\", y_maj, \"baseline_majority\")\n",
    "    acc_sgd, f1_sgd = summarize(\"SGD (partial_fit, balanced train)\", y_sgd, \"sgd_partialfit_balanced\")\n",
    "    acc_xgb, f1_xgb = summarize(\"XGBoost (incremental, balanced train)\", y_xgb, \"xgb_incremental_balanced\")\n",
    "\n",
    "    # ----- Feature importance -----\n",
    "    # SGD: numeric explicit + hashed cats aggregated (renamed)\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        coef = clf.coef_\n",
    "        imp = np.linalg.norm(coef, axis=0)\n",
    "        n_num = len(NUM_COLS)\n",
    "        fi_sgd = (pd.DataFrame({\n",
    "            \"feature\": NUM_COLS + [HASHED_LABEL],\n",
    "            \"importance\": list(imp[:n_num]) + [float(imp[n_num:].sum())],\n",
    "        }).sort_values(\"importance\", ascending=False))\n",
    "        fi_sgd.to_csv(os.path.join(RESULTS_ROOT, \"t7_feat_importance_sgd_balanced.csv\"), index=False)\n",
    "        plt.figure(figsize=(7.2, 3.8))\n",
    "        sns.barplot(data=fi_sgd.sort_values(\"importance\", ascending=True),\n",
    "                    x=\"importance\", y=\"feature\", orient=\"h\")\n",
    "        plt.title(\"Feature importance — SGD (numeric explicit; hashed cats aggregated)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIG_DIR, \"t7_featimp_sgd_balanced.pdf\"))\n",
    "        plt.close()\n",
    "\n",
    "    # XGB: gain-based; map f0.. to column indices (renamed)\n",
    "    if booster is not None:\n",
    "        gains = np.zeros(len(NUM_COLS) + N_HASH, dtype=float)\n",
    "        for k, v in booster.get_score(importance_type=\"gain\").items():\n",
    "            idx = int(k[1:])  # 'f123' -> 123\n",
    "            if 0 <= idx < gains.size:\n",
    "                gains[idx] += float(v)\n",
    "        fi_xgb = (pd.DataFrame({\n",
    "            \"feature\": NUM_COLS + [HASHED_LABEL],\n",
    "            \"importance\": list(gains[:len(NUM_COLS)]) + [float(gains[len(NUM_COLS):].sum())],\n",
    "        }).sort_values(\"importance\", ascending=False))\n",
    "        fi_xgb.to_csv(os.path.join(RESULTS_ROOT, \"t7_feat_importance_xgb_balanced.csv\"), index=False)\n",
    "        plt.figure(figsize=(7.2, 3.8))\n",
    "        sns.barplot(data=fi_xgb.sort_values(\"importance\", ascending=True),\n",
    "                    x=\"importance\", y=\"feature\", orient=\"h\")\n",
    "        plt.title(\"Feature importance — XGBoost (numeric explicit; hashed cats aggregated)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIG_DIR, \"t7_featimp_xgb_balanced.pdf\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Save summary — macro avg accuracy everywhere\n",
    "    pd.DataFrame([\n",
    "        {\"model\": \"Majority baseline\", \"macro_accuracy\": acc_maj, \"macro_f1\": f1_maj},\n",
    "        {\"model\": \"SGD(partial_fit, balanced)\", \"macro_accuracy\": acc_sgd, \"macro_f1\": f1_sgd},\n",
    "        {\"model\": \"XGB(incremental, balanced)\", \"macro_accuracy\": acc_xgb, \"macro_f1\": f1_xgb},\n",
    "    ]).to_csv(os.path.join(RESULTS_ROOT, \"t7_service_classifier_results.csv\"), index=False)\n",
    "\n",
    "    return clf, booster\n",
    "\n",
    "# ---------- convenience ----------\n",
    "def year_parts(service: str, year: int) -> List[str]:\n",
    "    ydir = os.path.join(TASK2_OUT_ROOT, SERVICE_DIR[service], f\"year={year}\")\n",
    "    if not os.path.isdir(ydir):\n",
    "        return []\n",
    "    return sorted(glob(os.path.join(ydir, \"part.*.parquet\")))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_files = {\n",
    "        \"Yellow\": year_parts(\"Yellow\", 2023),\n",
    "        \"Green\":  year_parts(\"Green\",  2023),\n",
    "        \"FHVHV\":  year_parts(\"FHVHV\",  2023),\n",
    "    }\n",
    "    valid_files = {\n",
    "        \"Yellow\": year_parts(\"Yellow\", 2024),\n",
    "        \"Green\":  year_parts(\"Green\",  2024),\n",
    "        \"FHVHV\":  year_parts(\"FHVHV\",  2024),\n",
    "    }\n",
    "\n",
    "    # Optional caps for quick prototyping; set to {} or None for full runs\n",
    "    caps = {\"FHVHV\": 50_000, \"Yellow\": 50_000, \"Green\": 50_000}\n",
    "    # caps = None\n",
    "\n",
    "    sgd, booster = train_and_eval(train_files, valid_files, cap_per_file=caps)\n",
    "    print(\"\\n[OK] Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc77409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>speed_mph</th>\n",
       "      <th>hour</th>\n",
       "      <th>dow</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FHVHV</td>\n",
       "      <td>7.040</td>\n",
       "      <td>35.466667</td>\n",
       "      <td>11.909774</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FHVHV</td>\n",
       "      <td>1.880</td>\n",
       "      <td>13.183333</td>\n",
       "      <td>8.556258</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>21.600</td>\n",
       "      <td>68.300000</td>\n",
       "      <td>18.975110</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Bronx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FHVHV</td>\n",
       "      <td>8.236</td>\n",
       "      <td>23.950000</td>\n",
       "      <td>20.632986</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green</td>\n",
       "      <td>1.160</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>4.176000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>0.710</td>\n",
       "      <td>4.116667</td>\n",
       "      <td>10.348178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>16.670</td>\n",
       "      <td>32.966667</td>\n",
       "      <td>30.339737</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FHVHV</td>\n",
       "      <td>0.980</td>\n",
       "      <td>4.566667</td>\n",
       "      <td>12.875913</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Green</td>\n",
       "      <td>3.140</td>\n",
       "      <td>16.383333</td>\n",
       "      <td>11.499492</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Green</td>\n",
       "      <td>3.010</td>\n",
       "      <td>18.983333</td>\n",
       "      <td>9.513608</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>1.700</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>12.592593</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Green</td>\n",
       "      <td>1.260</td>\n",
       "      <td>12.083333</td>\n",
       "      <td>6.256552</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   service  trip_distance  duration_min  speed_mph  hour  dow pickup_borough  \\\n",
       "0    FHVHV          7.040     35.466667  11.909774    18    5      Manhattan   \n",
       "1    FHVHV          1.880     13.183333   8.556258    21    4       Brooklyn   \n",
       "2   Yellow         21.600     68.300000  18.975110    10    2         Queens   \n",
       "3    FHVHV          8.236     23.950000  20.632986     2    6       Brooklyn   \n",
       "4    Green          1.160     16.666667   4.176000     8    2      Manhattan   \n",
       "5   Yellow          0.710      4.116667  10.348178     0    0      Manhattan   \n",
       "6   Yellow         16.670     32.966667  30.339737    19    2         Queens   \n",
       "7    FHVHV          0.980      4.566667  12.875913    22    3         Queens   \n",
       "8    Green          3.140     16.383333  11.499492    19    0       Brooklyn   \n",
       "9    Green          3.010     18.983333   9.513608    20    3       Brooklyn   \n",
       "10  Yellow          1.700      8.100000  12.592593     0    0      Manhattan   \n",
       "11   Green          1.260     12.083333   6.256552     8    3         Queens   \n",
       "\n",
       "   dropoff_borough  \n",
       "0        Manhattan  \n",
       "1         Brooklyn  \n",
       "2            Bronx  \n",
       "3         Brooklyn  \n",
       "4        Manhattan  \n",
       "5        Manhattan  \n",
       "6         Brooklyn  \n",
       "7           Queens  \n",
       "8        Manhattan  \n",
       "9         Brooklyn  \n",
       "10       Manhattan  \n",
       "11          Queens  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(balanced_batches(valid_files, caps))\n",
    "b\n",
    "# Xv, yv = _design(b)\n",
    "# Xv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd-project-25 (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
