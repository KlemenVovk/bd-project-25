{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5b81e6",
   "metadata": {},
   "source": [
    "# Kafka streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde3b46",
   "metadata": {},
   "source": [
    "First, we modified the docker-compose file to include an additional container for hosing jupyter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b99016",
   "metadata": {},
   "source": [
    "  jupyter:\n",
    "    image: python:3.9-slim\n",
    "    container_name: jupyter-kafka\n",
    "    platform: linux/amd64\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "    volumes:\n",
    "      - ./jupyter:/workspace\n",
    "    working_dir: /workspace\n",
    "    command: >\n",
    "      bash -c \"pip install jupyterlab pandas scikit-learn pyarrow faust confluent-kafka plotly bytewax &&\n",
    "               jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --NotebookApp.token=''\"\n",
    "    depends_on:\n",
    "      - broker1-kr\n",
    "      - broker2-kr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296aed67",
   "metadata": {},
   "source": [
    "We wrote a simple producer that loaded one of the .parquet files, ordered it by pickup date and produced a message every 0.5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab176ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "df = pd.read_parquet(\"./data/part.191.parquet\")\n",
    "df = df.sort_values(\"pickup_datetime\")\n",
    "\n",
    "# Kafka config\n",
    "conf = {\n",
    "    'bootstrap.servers': 'broker1-kr:9092'\n",
    "}\n",
    "producer = Producer(conf)\n",
    "\n",
    "topic = 'yellow_taxi_stream'\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    data = row.to_dict()\n",
    "\n",
    "    # Convert Timestamps to strings\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, pd.Timestamp):\n",
    "            data[key] = value.isoformat()\n",
    "\n",
    "    message = json.dumps(data)\n",
    "    producer.produce(topic, value=message)\n",
    "    producer.flush()\n",
    "    print(f\"Produced: {message}\")\n",
    "    time.sleep(0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a6487",
   "metadata": {},
   "source": [
    "We also wrote a simple consumer, that read from the same topic and printed out the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358c96c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import json\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': 'broker1-kr:9092',\n",
    "    'group.id': 'taxi_consumer_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['yellow_taxi_stream'])\n",
    "\n",
    "print(\"Consuming messages from 'yellow_taxi_stream'...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(\"Consumer error:\", msg.error())\n",
    "            continue\n",
    "\n",
    "        data = json.loads(msg.value().decode('utf-8'))\n",
    "        print(\"Consumed:\", data)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491bf04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We implemented a simple faust app, that calculates rolling statistics for each borough and writes it back to kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a490f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faust\n",
    "from typing import Optional\n",
    "from statistics import mean, stdev\n",
    "\n",
    "app = faust.App(\n",
    "    'taxi-stream-app',\n",
    "    broker='kafka://broker1-kr:9092',\n",
    "    value_serializer='json',\n",
    ")\n",
    "\n",
    "# Schema of incoming messages\n",
    "class TaxiRecord(faust.Record, serializer='json'):\n",
    "    total_amount: Optional[float]\n",
    "    passenger_count: Optional[int]\n",
    "\n",
    "# Schema for outgoing messages\n",
    "class StatsRecord(faust.Record, serializer='json'):\n",
    "    borough: str\n",
    "    mean_fare: float\n",
    "    std_fare: float\n",
    "    mean_psg: float\n",
    "    std_psg: float\n",
    "    mean_dist: float\n",
    "    std_dist: float\n",
    "    count: int\n",
    "\n",
    "\n",
    "\n",
    "# Topic from your producer\n",
    "taxi_topic = app.topic('yellow_taxi_stream', value_type=TaxiRecord)\n",
    "stats_topic = app.topic('yellow_taxi_stats', value_serializer='json')\n",
    "\n",
    "\n",
    "# Table for stats\n",
    "stats_table = app.Table(\n",
    "    'total_stats',\n",
    "    default=lambda: {\n",
    "        'count': 0,\n",
    "        'total_amounts': [],\n",
    "        'passengers': [],\n",
    "        'trip_distance': []\n",
    "        \n",
    "    },\n",
    "    partitions=1,\n",
    "    changelog_topic=app.topic('custom_stats_changelog', partitions=1)\n",
    ")\n",
    "\n",
    "\n",
    "@app.agent(taxi_topic)\n",
    "async def process(taxis):\n",
    "    async for taxi in taxis:\n",
    "        borough = taxi.pickup_borough\n",
    "        print(taxi.total_amount)\n",
    "        stats = stats_table[borough]  \n",
    "\n",
    "        # Update stats\n",
    "        stats['count'] += 1\n",
    "        stats['total_amounts'].append(taxi.total_amount or 0)\n",
    "        stats['passengers'].append(taxi.passenger_count or 0)\n",
    "        stats['trip_distance'].append(taxi.trip_distance or 0)\n",
    "\n",
    "        # Maintain a rolling window of last 100 values\n",
    "        for key in ['total_amounts', 'passengers', 'trip_distance']:\n",
    "            if len(stats[key]) > 100:\n",
    "                stats[key].pop(0)\n",
    "                \n",
    "        stats_table[borough] = stats\n",
    "        # Calculate and print rolling mean and std\n",
    "        if len(stats['total_amounts']) > 1:  # stdev needs at least 2 values\n",
    "            mean_fare = mean(stats['total_amounts'])\n",
    "            std_fare = stdev(stats['total_amounts'])\n",
    "            mean_psg = mean(stats['passengers'])\n",
    "            std_psg = stdev(stats['passengers'])\n",
    "            mean_dist = mean(stats['trip_distance'])\n",
    "            std_dist = stdev(stats['trip_distance'])\n",
    "            count = stats['count']\n",
    "            \n",
    "            print(f\"{borough} Count={stats['count']}\")\n",
    "            print(f\"  ðŸ’° Mean Fare: {mean_fare:.2f}, Std: {std_fare:.2f}\")\n",
    "            print(f\"  ðŸ‘¥ Mean Passengers: {mean_psg:.2f}, Std: {std_psg:.2f}\")\n",
    "            print(f\"  ðŸ“ Mean Distance: {mean_dist:.2f}, Std: {std_dist:.2f}\")\n",
    "\n",
    "                \n",
    "            stats_msg = StatsRecord(\n",
    "                borough=borough,\n",
    "                mean_fare=mean_fare,\n",
    "                std_fare=std_fare,\n",
    "                mean_psg=mean_psg,\n",
    "                std_psg=std_psg,\n",
    "                count=count,\n",
    "                mean_dist=mean_dist,\n",
    "                std_dist=std_dist\n",
    "            )\n",
    "    \n",
    "            await stats_topic.send(value=stats_msg)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981e6e5",
   "metadata": {},
   "source": [
    "For the stream clustering algorithm we decided to implement K-Means clustering. We update 3 initially chosen centroids based on the incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b07831",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faust\n",
    "import math\n",
    "from typing import Optional, List\n",
    "import numpy as np\n",
    "\n",
    "app = faust.App(\n",
    "    'taxi-cluster-app',\n",
    "    broker='kafka://broker1-kr:9092',\n",
    "    value_serializer='json',\n",
    ")\n",
    "\n",
    "class TaxiRecord(faust.Record, serializer='json'):\n",
    "    pickup_latitude: Optional[float]\n",
    "    pickup_longitude: Optional[float]\n",
    "    total_amount: Optional[float]\n",
    "    passenger_count: Optional[int]\n",
    "\n",
    "class ClusterCentroid(faust.Record, serializer='json'):\n",
    "    cluster_id: int\n",
    "    lng: float\n",
    "    lat: float\n",
    "\n",
    "taxi_topic = app.topic('yellow_taxi_stream', value_type=TaxiRecord)\n",
    "cluster_topic = app.topic('yellow_taxi_cluster_stream', value_type=ClusterCentroid)\n",
    "\n",
    "# Number of clusters\n",
    "K = 3\n",
    "\n",
    "# Initialize centroids arbitrarily (example coords)\n",
    "initial_centroids = [\n",
    "    {'lat': 40.7580, 'lon': -73.9855},  # Times Square approx\n",
    "    {'lat': 40.7128, 'lon': -74.0060},  # Lower Manhattan approx\n",
    "    {'lat': 40.730610, 'lon': -73.935242},  # East Village approx\n",
    "]\n",
    "\n",
    "# Table: key=cluster_id, value=dict with 'lat', 'lon', 'count'\n",
    "centroids = app.Table(\n",
    "    'centroids',\n",
    "    default=lambda: {'lat': 0.0, 'lon': 0.0, 'count': 0},\n",
    "    partitions=1,\n",
    "    changelog_topic=app.topic('custom_stats_changelog_centroid', partitions=1)\n",
    ")\n",
    "\n",
    "initialized = False  # Move this outside the agent, at module level\n",
    "\n",
    "@app.agent(taxi_topic)\n",
    "async def process(taxis):\n",
    "    global initialized  # to modify the external variable\n",
    "\n",
    "    async for taxi in taxis:\n",
    "        if not initialized:\n",
    "            for i, c in enumerate(initial_centroids):\n",
    "                centroids[i] = {'lat': c['lat'], 'lon': c['lon'], 'count': 0}\n",
    "            initialized = True  # only run once\n",
    "\n",
    "        if taxi.pickup_latitude is None or taxi.pickup_longitude is None or taxi.pickup_latitude==np.nan or taxi.pickup_longitude==np.nan:\n",
    "            continue\n",
    "\n",
    "        def distance(c, lat, lon):\n",
    "            return math.sqrt((c['lat'] - lat)**2 + (c['lon'] - lon)**2)\n",
    "\n",
    "        closest_id = min(\n",
    "            centroids.keys(),\n",
    "            key=lambda cid: distance(centroids[cid], taxi.pickup_latitude, taxi.pickup_longitude)\n",
    "        )\n",
    "\n",
    "        centroid = centroids[closest_id]\n",
    "        count = centroid['count']\n",
    "\n",
    "        new_count = count + 1\n",
    "        new_lat = (centroid['lat'] * count + float(taxi.pickup_latitude)) / new_count\n",
    "        new_lon = (centroid['lon'] * count + float(taxi.pickup_longitude)) / new_count\n",
    "\n",
    "        centroids[closest_id] = {'lat': new_lat, 'lon': new_lon, 'count': new_count}\n",
    "        print(float(taxi.pickup_longitude), float(taxi.pickup_latitude))\n",
    "        print(f\"Cluster {closest_id}: Lat {new_lat:.5f}, Lon {new_lon:.5f}, Count {new_count}\")\n",
    "\n",
    "        cluster_msg = ClusterCentroid(\n",
    "            cluster_id=closest_id,\n",
    "            lng=new_lon,\n",
    "            lat=new_lat\n",
    "        )\n",
    "        await cluster_topic.send(value=cluster_msg)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ac930",
   "metadata": {},
   "source": [
    "We also prepared a simple visualization using plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e39f2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import json\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "NUM_CLUSTERS = 3\n",
    "\n",
    "centroids = np.array([[40.75, -74.0], [40.73, -73.93], [40.70, -74.15]])\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': 'broker1-kr:9092',\n",
    "    'group.id': 'taxi_consumer_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['yellow_taxi_cluster_stream'])\n",
    "\n",
    "print(\"Consuming messages from 'yellow_taxi_cluster_stream'...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(\"Consumer error:\", msg.error())\n",
    "            continue\n",
    "\n",
    "        update = json.loads(msg.value().decode('utf-8'))\n",
    "        cid = update['cluster_id']\n",
    "        centroids[cid] = [update['lat'], update['lng']]\n",
    "    \n",
    "        df = pd.DataFrame(centroids, columns=[\"lat\", \"lon\"])\n",
    "        df[\"cluster\"] = [f\"Cluster {i}\" for i in range(NUM_CLUSTERS)]\n",
    "    \n",
    "        fig = px.scatter_mapbox(df, lat=\"lat\", lon=\"lon\", color=\"cluster\", zoom=9)\n",
    "        fig.update_layout(mapbox_style=\"carto-positron\",\n",
    "                          mapbox_center={\"lat\": 40.73, \"lon\": -73.98},\n",
    "                          margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    \n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "        time.sleep(3)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5d4d9",
   "metadata": {},
   "source": [
    "Lastly we used bytewax python library to calculate rolling statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bad0c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bytewax import operators as op\n",
    "from bytewax.dataflow import Dataflow\n",
    "from bytewax.connectors.kafka import KafkaSource\n",
    "from bytewax.connectors.stdio import StdOutSink\n",
    "import json\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "brokers = [\"broker1-kr:9092\"]\n",
    "flow = Dataflow(\"rolling_avg_per_borough\")\n",
    "\n",
    "# Kafka input\n",
    "stream = op.input(\"in\", flow, KafkaSource(brokers, [\"yellow_taxi_stream\"]))\n",
    "\n",
    "# Parse and key by borough\n",
    "keyed = op.key_on(\"key_by_borough\", stream, lambda msg: msg.key.decode(\"utf-8\"))\n",
    "keyed = op.map_value(\"parse_json\", keyed, lambda msg: json.loads(msg.value.decode(\"utf-8\")))\n",
    "keyed = op.map_value(\"get_amount\", keyed, lambda msg: float(msg[\"total_amount\"]))\n",
    "\n",
    "# Rolling average calculation\n",
    "def rolling_avg(state, new_value):\n",
    "    if state is None:\n",
    "        state = []\n",
    "\n",
    "    print(\"Before state:\", state)\n",
    "    print(\"New value:\", new_value)\n",
    "\n",
    "    state.append(new_value)\n",
    "    if len(state) > WINDOW_SIZE:\n",
    "        state.pop(0)\n",
    "\n",
    "    print(\"After state:\", state)\n",
    "    avg = round(sum(state) / len(state), 2)\n",
    "    print(\"Running avg:\", avg)\n",
    "    print()\n",
    "\n",
    "    return (state, avg)\n",
    "\n",
    "# Apply rolling average per key\n",
    "rolling_avgs = op.stateful_map(\"rolling_avg\", keyed, rolling_avg)\n",
    "\n",
    "# Output to stdout\n",
    "op.output(\"print_out\", rolling_avgs, StdOutSink())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
