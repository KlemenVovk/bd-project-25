{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5246a0",
   "metadata": {},
   "source": [
    "### A notebook version of 01_data_ingestion.py for explanation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f42c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from glob import glob\n",
    "from dask.distributed import LocalCluster, Client\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "import dask\n",
    "import os\n",
    "from constants import RAW_DATA_ROOT, YEARS, TASK1_OUT_ROOT, TAXI_ZONES_SHAPEFILE, ZONES_TO_CENTROIDS_MAPPING_CSV, TASK1_SCHEMA, TASK1_NP_SCHEMA, COLUMN_CONSISTENCY_NAMING_MAP\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_raw_files(root_path, year):\n",
    "    \"\"\"\n",
    "    Get all original files in the given path for the specified year.\n",
    "    \"\"\"\n",
    "    return sorted(list(glob(f\"{root_path}/yellow_tripdata_{year}*.parquet\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c60b48",
   "metadata": {},
   "source": [
    "### The goal\n",
    "Given raw parquet files from 2009 to 2025, we want to concat them along the time axis (mapping the columns for consistency in terms of values and names) and save the results in different file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c04b6",
   "metadata": {},
   "source": [
    "### Renaming columns for consistency\n",
    "\n",
    "```py\n",
    "COLUMN_CONSISTENCY_NAMING_MAP = {\n",
    "    \"End_Lat\": \"dropoff_latitude\",\n",
    "    \"End_Lon\": \"dropoff_longitude\",\n",
    "    \"Start_Lat\": \"pickup_latitude\",\n",
    "    \"Start_Lon\": \"pickup_longitude\",\n",
    "    \"Fare_Amt\": \"fare_amount\",\n",
    "    \"Tip_Amt\": \"tip_amount\",\n",
    "    \"Tolls_Amt\": \"tolls_amount\",\n",
    "    \"Total_Amt\": \"total_amount\",\n",
    "    \"Passenger_Count\": \"passenger_count\",\n",
    "    \"Payment_Type\": \"payment_type\",\n",
    "    \"Rate_Code\": \"rate_code_id\",\n",
    "    \"rate_code\": \"rate_code_id\",\n",
    "    \"RatecodeID\": \"rate_code_id\",\n",
    "    \"Trip_Distance\": \"trip_distance\",\n",
    "    \"Trip_Dropoff_DateTime\": \"tpep_dropoff_datetime\",\n",
    "    \"Trip_Pickup_DateTime\": \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "    \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "    \"Airport_fee\": \"airport_fee\",\n",
    "    \"VendorID\": \"vendor_id\",\n",
    "    \"vendor_name\": \"vendor_id\",\n",
    "    \"surcharge\": \"extra\",\n",
    "    \"store_and_forward\": \"store_and_fwd_flag\",\n",
    "}\n",
    "```\n",
    "\n",
    "With time, datasets changed column names: \n",
    "- Some of the changes are trivial (letter case, acronyms etc.)\n",
    "- Some have been replaced by alternatives (e.g. dropoff latitude and longitude have been replaced by a DOLocationID which maps the dropoff location to a taxi zone in NYC, losing some granularity).\n",
    "- Some have been newly added (e.g. `congestion_surcharge`, `improvement_surcharge`, `airport_fee`). We add these columns where they are missing.\n",
    "- some columns have changed the valid values over the years\n",
    "    - vendor_id - currently valid vendors are denoted as 1,2,6, or 7. Early datasets have vendor acronyms (e.g. CMT, DDS...). We map those to the appropriate numbers (over the years, the same vendor has been merged/absorbed into larger companies). Where mapping was not possible we impute -1. This is done so that we can identify these rows later because we don't know if we will need vendor ids at all - if not, it would be unwise to throw those rows away.\n",
    "    - rate_code_id - currently valid values are 1,2,3,4,5,6 and 99 - 99 marks \"invalid\" so where we had missing values, or invalid numbers, we imputed 99 to stay consistent.\n",
    "    - store_and_fwd_flag - over the years it has been stored as Y or N or 1 or 0 or 1.0 and NA. We map everything to 1 and 0, where ambigous we impute -1\n",
    "    - payment_type - over the years it has been stored as acronyms (CAS) or full word (CASH) or numbers. Currently valid values are 0,1,2,3,4,5,6 with 5 meaning Unknown. So where ambigous we impute 5.\n",
    "    - any additional fees, charges (like `congestion_surcharge` or `airport_fee`) have imputed 0 wherever values where missing, as it doesn't change the total amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33092d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_vendor(value):\n",
    "    # vendor_id (previously vendor_name but stands for the same thing)\n",
    "    _old_vendorname2id = {\n",
    "        \"CMT\": 1,\n",
    "        \"DDS\": 2, # DDS and VTS were merged (Verifone) and later also merged with Curb Mobility\n",
    "        \"VTS\": 2,\n",
    "    }\n",
    "    valid_vendor_ids= [1, 2, 6, 7]\n",
    "    if str(value) in _old_vendorname2id:\n",
    "        return _old_vendorname2id[value]\n",
    "    if float(value) in valid_vendor_ids:\n",
    "        return int(value)\n",
    "    return -1 # Invalid vendor id\n",
    "    \n",
    "def map_rate_code(value):\n",
    "    # rate_code_id\n",
    "    # 1-6 and 99 (missing or unknown). All other values are set to 99\n",
    "    valid_rate_code_ids = [1, 2, 3, 4, 5, 6, 99]\n",
    "    if float(value) in valid_rate_code_ids:\n",
    "        return int(value)\n",
    "    return 99 # Invalid rate code id\n",
    "    \n",
    "def map_store_and_fwd_flag(value):\n",
    "    # store_and_fwd_flag\n",
    "    # npnan to \"NA\"\n",
    "    letter_mapping = {\n",
    "        \"Y\": 1,\n",
    "        \"N\": 0,\n",
    "    }\n",
    "    if str(value).strip() in letter_mapping:\n",
    "        return letter_mapping[str(value)]\n",
    "    try:\n",
    "        v = int(float(value))\n",
    "        if v in [0, 1]:\n",
    "            return v\n",
    "    except:\n",
    "        pass\n",
    "    return -1\n",
    "\n",
    "def map_payment_type(value):\n",
    "    # ignore case and strip\n",
    "    payment_type_mapping = {\n",
    "        \"credit\": 1,\n",
    "        \"crd\": 1,\n",
    "        \"cre\":1,\n",
    "        \"cash\": 2,\n",
    "        \"csh\": 2,\n",
    "        \"cas\": 2,\n",
    "        \"noc\": 3,\n",
    "        \"no charge\": 3,\n",
    "        \"no\": 3,\n",
    "        \"dis\": 4,\n",
    "        \"dispute\": 4,\n",
    "        \"na\": 5,\n",
    "    }\n",
    "    valid_vals = [0, 1, 2, 3, 4, 5, 6]\n",
    "    if str(value).strip().lower() in payment_type_mapping:\n",
    "        return payment_type_mapping[str(value).strip().lower()]\n",
    "    if float(value) in valid_vals:\n",
    "        return int(float(value))\n",
    "    return 5 # unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703733e",
   "metadata": {},
   "source": [
    "### Mapping LocationIDs to geographical coordinates\n",
    "\n",
    "- Newer datasets do not provide latitude and longitude of pickup and dropoff. Instead they provide a PULocationID and DOLocationID which indicates a taxi zone in NYC.\n",
    "- To stay consistent over the years,  we map the LocationIDs to geographical lat,lng center points of zones (shapefiles of zones are available on the same site as the original dataset and downloaded with `download_zone_data.sh`). Older datasets which have exact lat,lng locations are left as is\n",
    "- This way we stay as granular as possible and can decide on the granularity later.\n",
    "- LocationIDs of 264 and 265 mark \"Unknown or invalid or not provided\". We impute NaN when mapping these to lat,lng coordinates to prevent accidental spatial joins with (0,0) coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locationid_to_centroid(shapefile):\n",
    "    # Load zones as GeoDataFrame\n",
    "    zones = gpd.read_file(shapefile)\n",
    "\n",
    "    # Reproject to NYC's local projected CRS (US feet) for correct geometry math\n",
    "    zones_projected = zones.to_crs(\"EPSG:2263\")\n",
    "\n",
    "    # Calculate centroid in projected space\n",
    "    zones_projected['centroid'] = zones_projected.geometry.centroid\n",
    "\n",
    "    # Convert centroid back to WGS84 (lat/lon)\n",
    "    centroids_wgs84 = zones_projected.set_geometry('centroid').to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Extract lat/lng from centroid geometry\n",
    "    zones['centroid_lat'] = centroids_wgs84.geometry.y\n",
    "    zones['centroid_lng'] = centroids_wgs84.geometry.x\n",
    "\n",
    "    zones = zones[['OBJECTID', 'centroid_lat', 'centroid_lng']]\n",
    "    zones['centroid_lat'] = zones['centroid_lat'].astype(float)\n",
    "    zones['centroid_lng'] = zones['centroid_lng'].astype(float)\n",
    "    zones['OBJECTID'] = zones['OBJECTID'].astype(int)\n",
    "\n",
    "    # create a pandas dataframe with the same columns\n",
    "    # Apparently, there are duplicate LocationIDs that are mapped to unique Objectids so the matching later doesnt work.\n",
    "    # Have checked the taxi zone map and visually it is ok (some locationids are shown on the map, that aren't in the LocationID colmun, but are in the objectid column)\n",
    "    # I.e. ObjectID=56, and 57 are present but both are mapped to LocationID=56 so the join fails...\n",
    "    zones = pd.DataFrame(zones[['OBJECTID', 'centroid_lat', 'centroid_lng']].copy())\n",
    "    zones = zones.rename(columns={'OBJECTID': 'LocationID'})\n",
    "    return zones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cc47a",
   "metadata": {},
   "source": [
    "### Saving to different formats\n",
    "\n",
    "- We provide functions to save as Parquet, CSV or HDF5.\n",
    "- We optimize the data types and define them in constants.py for each column.\n",
    "- Since as per instructions we are writing to a single file, paralelization at the disk level is not possible (race conditions), however it would not make much differnece anyway as the main bottleneck is disk speed (I/O) - if the disk can write with 500MB/s it will do that to one file at full speed or 10 files, each at 1/10th of the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_parquet_sequentially(dfs, root_dir, partition_on=None, schema=None):\n",
    "    # Write the first file, as parquet can only append if a dataset already exists.\n",
    "    assert isinstance(dfs, list), \"dfs must be a list of dataframes\"\n",
    "    assert len(dfs) > 0, \"No dataframes to write\"\n",
    "    dfs[0].to_parquet(\n",
    "        path=root_dir,\n",
    "        partition_on=partition_on,\n",
    "        engine='pyarrow',\n",
    "        schema=schema,\n",
    "        write_index=False,\n",
    "        compute=True,\n",
    "    )\n",
    "\n",
    "    # Write the rest of the dataframes\n",
    "    for i in tqdm(range(1, len(dfs))):\n",
    "        dfs[i].to_parquet(\n",
    "            path=root_dir,\n",
    "            partition_on=partition_on,\n",
    "            engine='pyarrow',\n",
    "            schema=schema,\n",
    "            write_index=False,\n",
    "            append=True,\n",
    "            compute=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def write_csv_sequentially(dfs, csv_file):\n",
    "    for i in tqdm(range(len(dfs))):\n",
    "        dfs[i].to_csv(\n",
    "            header=(i == 0), # write header only for the first file\n",
    "            filename=csv_file,\n",
    "            single_file=True,\n",
    "            index=False,\n",
    "            mode=\"wt\" if i == 0 else \"a\", # append/create if not exists\n",
    "            compute=True\n",
    "        )\n",
    "\n",
    "def write_csv_parallel_concat(dfs, csv_file):\n",
    "    df_concat = dd.concat(dfs, axis=0)\n",
    "    df_concat.to_csv(\n",
    "        header=True,\n",
    "        filename=csv_file,\n",
    "        single_file=True,\n",
    "        index=False,\n",
    "        mode=\"wt\", # append/create if not exists\n",
    "        compute=True\n",
    "    )\n",
    "\n",
    "def write_hdf5_parallel_concat(dfs, hdf_filepath):\n",
    "    df = dd.concat(dfs, axis=0)\n",
    "    # convert datetimes to int64 (h5py can't handle datetimes directly)\n",
    "    for col in df.select_dtypes(include=['datetime64[ns]']):\n",
    "        df[col] = df[col].astype('int64')\n",
    "\n",
    "    df = df.compute()\n",
    "    # to numpy (structured array) as h5py can't handle dataframes directly\n",
    "    dtype = np.dtype([(col, df[col].dtype) for col in df.columns])\n",
    "    structured_array = np.empty(len(df), dtype=dtype)\n",
    "    for col in df.columns:\n",
    "        structured_array[col] = df[col].to_numpy()\n",
    "\n",
    "    # write as single dataset\n",
    "    with h5py.File(hdf_filepath, 'w') as h5f:\n",
    "        h5f.create_dataset('taxidata', data=structured_array, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d837a",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local cluster\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "print(cluster.dashboard_link)\n",
    "\n",
    "# # SLURM cluster\n",
    "# cluster = SLURMCluster(\n",
    "#     cores=4,\n",
    "#     processes=1,\n",
    "#     memory=\"100GB\",\n",
    "#     walltime=\"12:00:00\",\n",
    "#     death_timeout=600,\n",
    "# )\n",
    "# cluster.adapt(minimum=1, maximum=2)\n",
    "# print(cluster.job_script())\n",
    "# client = Client(cluster)\n",
    "\n",
    "\n",
    "print(\"Reading files...\")\n",
    "files = sum((get_raw_files(RAW_DATA_ROOT, year) for year in YEARS), [])\n",
    "dfs = [dd.read_parquet(f) for f in files]\n",
    "\n",
    "print(\"Renaming columns...\")\n",
    "# Renaming columns for consistency\n",
    "for i, f in enumerate(files):\n",
    "    # Rename columns\n",
    "    for old_col, new_col in COLUMN_CONSISTENCY_NAMING_MAP.items():\n",
    "        if old_col in dfs[i].columns.tolist():\n",
    "            dfs[i] = dfs[i].rename(columns={old_col: new_col})\n",
    "\n",
    "# Fix column values (mapping, dtypes...) to be able to concat.\n",
    "print(\"Fixing column values (mapping to consistent values, dtypes, locationids to (lat,lng) centroids etc.)...\")\n",
    "locationid_to_centers_df = get_locationid_to_centroid(TAXI_ZONES_SHAPEFILE).sort_values(by='LocationID', ascending=True, ignore_index=True)\n",
    "locationid_to_centers_df.to_csv(ZONES_TO_CENTROIDS_MAPPING_CSV, index=False)\n",
    "for i, f in tqdm(enumerate(files)):\n",
    "    # map vendor_id column but with dask so i can compute later\n",
    "    dfs[i]['vendor_id'] = dfs[i]['vendor_id'].map(map_vendor, meta=('vendor_id', 'int8'))\n",
    "    dfs[i]['rate_code_id'] = dfs[i]['rate_code_id'].map(map_rate_code, meta=('rate_code_id', 'int8'))\n",
    "    dfs[i]['store_and_fwd_flag'] = dfs[i]['store_and_fwd_flag'].map(map_store_and_fwd_flag, meta=('store_and_fwd_flag', 'int8'))\n",
    "    dfs[i]['payment_type'] = dfs[i]['payment_type'].map(map_payment_type, meta=('payment_type', 'int8'))\n",
    "    # pickup\n",
    "    if \"PULocationID\" in dfs[i].columns:\n",
    "        # set dtype to int\n",
    "        dfs[i]['PULocationID'] = dfs[i]['PULocationID'].astype(int)\n",
    "        # show any pulocationds that are not in the locationid_to_centers_df\n",
    "        dfs[i] = dfs[i].merge(locationid_to_centers_df, how='left', left_on='PULocationID', right_on='LocationID')\n",
    "        # rename to pickup lattitude and longitude\n",
    "        dfs[i] = dfs[i].rename(columns={'centroid_lat': 'pickup_latitude', 'centroid_lng': 'pickup_longitude'})\n",
    "\n",
    "    # dropoff\n",
    "    if \"DOLocationID\" in dfs[i].columns:\n",
    "        # set dtype to int\n",
    "        dfs[i]['DOLocationID'] = dfs[i]['DOLocationID'].astype(int)\n",
    "        dfs[i] = dfs[i].merge(locationid_to_centers_df, how='left', left_on='DOLocationID', right_on='LocationID')\n",
    "        # rename to dropoff lattitude and longitude\n",
    "        dfs[i] = dfs[i].rename(columns={'centroid_lat': 'dropoff_latitude', 'centroid_lng': 'dropoff_longitude'})\n",
    "    # drop LocationID columns\n",
    "    dfs[i] = dfs[i].drop(columns=['DOLocationID', 'PULocationID', 'LocationID_x', 'LocationID_y'], errors='ignore')\n",
    "\n",
    "    if 'airport_fee' not in dfs[i].columns:\n",
    "        # set airport_fee to 0\n",
    "        dfs[i]['airport_fee'] = 0.0\n",
    "    if 'congestion_surcharge' not in dfs[i].columns:\n",
    "        dfs[i]['congestion_surcharge'] = 0.0\n",
    "    if 'improvement_surcharge' not in dfs[i].columns:\n",
    "        dfs[i]['improvement_surcharge'] = 0.0\n",
    "    year = int(os.path.basename(f).split('_')[2][:4])\n",
    "    dfs[i]['year'] = year\n",
    "    dfs[i]['year'] = dfs[i]['year'].astype(np.int16)\n",
    "    dfs[i]['mta_tax'] = dfs[i]['mta_tax'].fillna(0.0)\n",
    "    # parse datetimes\n",
    "    dfs[i]['pickup_datetime'] = dd.to_datetime(dfs[i]['pickup_datetime'], errors='raise')\n",
    "    dfs[i]['dropoff_datetime'] = dd.to_datetime(dfs[i]['dropoff_datetime'], errors='raise')\n",
    "    dfs[i]['airport_fee'] = dfs[i]['airport_fee'].fillna(0.0).astype(float)\n",
    "    dfs[i]['congestion_surcharge'] = dfs[i]['congestion_surcharge'].fillna(0.0).astype(float)\n",
    "    dfs[i]['improvement_surcharge'] = dfs[i]['improvement_surcharge'].fillna(0.0).astype(float)\n",
    "    dfs[i]['extra'] = dfs[i]['extra'].fillna(0.0).astype(float)\n",
    "    dfs[i]['passenger_count'] = dfs[i]['passenger_count'].fillna(0).astype(int)\n",
    "    # Fix dtypes\n",
    "    dfs[i] = dfs[i].astype(TASK1_NP_SCHEMA)\n",
    "    # print shape of each dataframe and the corresponding file\n",
    "\n",
    "#! Up to here, everything is embarrasingly parallelizable (a `dask.compute(*dfs)` would work in parallel - tested).\n",
    "#! We are saving files sequentially, since multiprocessing append doesn't really work that well in to_parquet or when writing to the same CSV file.\n",
    "#! Furthermore, even if we do parallelize, the bottleneck is in disk I/O.\n",
    "\n",
    "# Compute everything in parallel (persist)\n",
    "# dfs = client.persist(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e08cf0",
   "metadata": {},
   "source": [
    "### Saving everything to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad01698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing files...\")\n",
    "# Create needed folders\n",
    "os.makedirs(os.path.join(TASK1_OUT_ROOT, \"one_year\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TASK1_OUT_ROOT, \"five_years\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TASK1_OUT_ROOT, \"all\"), exist_ok=True)\n",
    "\n",
    "# Write parquet (all)\n",
    "parquet_root = os.path.join(TASK1_OUT_ROOT, \"all\")\n",
    "print(f\"Writing Parquet (all) to {parquet_root}...\")\n",
    "write_parquet_sequentially(dfs, parquet_root, partition_on=['year'], schema=TASK1_SCHEMA)\n",
    "\n",
    "# CSV (5 years)\n",
    "csv_file = os.path.join(TASK1_OUT_ROOT, \"five_years\", \"2020_2024.csv\")\n",
    "print(f\"Writing CSV (5 years) to {csv_file}...\")\n",
    "write_csv_sequentially(dfs[-61:-1], csv_file)\n",
    "\n",
    "# CSV (1 year)\n",
    "csv_file = os.path.join(TASK1_OUT_ROOT, \"one_year\", \"2024.csv\")\n",
    "print(f\"Writing CSV (1 year) to {csv_file}...\")\n",
    "write_csv_sequentially(dfs[-13:-1], csv_file) \n",
    "\n",
    "# HDF (1 year)\n",
    "# as a proof of concept - we can concat in parallel, an then write to a single file, however this just uses a lot of memory and is not really needed.\n",
    "# since the bottleneck is writing to disk not computation and then workers spend a lot of time just waiting.\n",
    "# using h5py as dask's implementation has problems (it can only save in the \"tables\" format resulting in a larger file than the equivalent CSV...)\n",
    "hdf_file = os.path.join(TASK1_OUT_ROOT, \"one_year\", \"2024.h5\")\n",
    "print(f\"Writing HDF5 (1 year) to {hdf_file}...\")\n",
    "write_hdf5_parallel_concat(dfs[-13:-1], hdf_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
